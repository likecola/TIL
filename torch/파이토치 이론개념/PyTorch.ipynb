{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch의 특징: \n",
    "1. Numpy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n",
    "2. 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)\n",
    "\n",
    "## Tensor\n",
    "PyTorch의 가장 핵심적인 개념\n",
    "텐서는 개념적으로 NumPy 배열과 동일 : \n",
    "Tensor는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 기능 제공\n",
    "NumPy와 다르게 GPU를 사용하여 수치 연산을 가속할 수 있다.\n",
    "\n",
    "## Autograd\n",
    "### 텐서(Tensor)와 autograd\n",
    "autograd 패키지는 신경망의 순전파 단계에서 연산 그래프를 정의\n",
    "이 그래프의 노드는 tensor이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를 만드는 함수가 됨\n",
    "이 그래프를 통해 역전파를 하면 변화도를 쉽게 계산할 수 있음\n",
    "\n",
    "각 텐서는 연산그래프에서 노드로 표현\n",
    "```\n",
    "requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "```\n",
    "\n",
    "\n",
    "### 새 autograd Function 정의하기\n",
    "- 내부적으로, autograd의 기본 연산자는 실제로 텐서를 조작하는 2개의 함수\n",
    "- forward함수는 입력 텐서로부터 출력 텐서를 계산\n",
    "- backward 함수는 어떤 스칼라 값에 대한 출력 텐서의 변화도(gradient)를 전달받고, 동일한 스칼라 값에 대한 입력 텐서의 변화도를 계산\n",
    "\n",
    "PyTorch에서 `torch.autograd.Function` 의 하위클래스(subclass)를 정의하고 `forward` 와 `backward` 함수를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게 정의할 수 있다.\n",
    "\n",
    "그 후, 인스턴스를 생성하고 이를 함수처럼 호출하고, 입력 데이터를 갖는 텐서를 전달하는 \n",
    "식으로 새로운 autograd 연산자를 사용\n",
    "\n",
    "\n",
    "## nn 모듈\n",
    "- 연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수를 자동으로 계산하는 매우 강력한 패러다임이나 대규모 신경망에서는 너무 low-level일 수 있음\n",
    "- 신경망을 구성하는 것을 종종 연산을 계층(layer)에 배열하는 것으로 생각하는데,\n",
    "이 중 일부는 학습 도중 최적화가 될 학습 가능한 매개변수를 가지고 있음\n",
    "\n",
    "- 텐서플로우에서는 Keras와 TensorFlow-Slim, TFlearn 같은 패키지들이 연산 그래프를 고수준으(high-level)으로 추상화하여 제공하므로 신경망을 구축하는데 유용\n",
    "- nn 패키지는 신경망 계층과 거의 비슷한 Module의 집합을 정의\n",
    "- Module은 입력 텐서를 받고 출력 텐서를 계산하는 한 편, 학습 가능한 매개변수를 갖는 텐서들을 내부 상태로 가짐\n",
    "- nn 패키지는 또한 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수도 정의하고 있음\n",
    "\n",
    "## optim\n",
    "실제로 신경망을 학습할 때는 AdaGrad, RMSProp, Adam 과 같은 \n",
    "더 정교한 옵티마이저를 사용\n",
    "\n",
    "optim 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 \n",
    "최적화 알고리즘의 구현체를 제공\n",
    "\n",
    "### 사용자 정의 nn.Module\n",
    "nn.Module의 하위 클래스로 새로운 Module을 정의하고,\n",
    "입력 텐서를 받아 다른 모듈 및 autograd 연산을 사용하여 출력 텐서를 만드는 forward를 정의\n",
    "\n",
    "\n",
    "### 제어 흐름(Control Flow) + 가중치 공유 (Weight Sharing)\n",
    "- 일반적인 Python 제어 흐름을 사용하여 반복을 구현할 수 있으며,\n",
    "순전파 단계를 정의할 때 동일한 매개변수를 여러번 재사용하여 가중치 공유를 구현\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
