{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ernbLmlz7Sel",
        "outputId": "5df30396-bf90-490c-f62e-09a5f58d8062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 살펴보기"
      ],
      "metadata": {
        "id": "mhmS9n5O8Ufb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/data/ArticlesApril2017.csv\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLl0Ms-8Uyk",
        "outputId": "cade64ba-4879-4886-8393-bf283ec60ab3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습용 데이터셋 정의\n",
        "- BOW를 만드는 과정\n",
        "- 데이터에 특수문자 제거 => 단어마다 고유 번호를 매김"
      ],
      "metadata": {
        "id": "hmNE93hy8mIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "\n",
        "        # ❶ 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"/content/drive/MyDrive/data/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # ❷ headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # ❸ 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # ➍ 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus:\n",
        "            for word in line.split():\n",
        "                if word not in self.BOW.keys():\n",
        "                    self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split()\n",
        "            line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2])\n",
        "            for i in range(len(line_bow)-2)]\n",
        "\n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0])  # ❶ 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # ❷ 출력 데이터\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "H2Qo4oxI8kU7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM모델 정의\n",
        "- 자연어 처리에 사용하는 단어 개수는 다중분류의 클래스보다 더 많습니다.\n",
        "- 모델의 입력으로 들어가는 입려값의 대부분이 0이 되어버리면(희소표현) 학습이 원활하게 이뤄지지 않습니다.\n",
        "- 그래서 임베딩이 필요합니다. 임베딩층은 희소 표현인 입력 벡터를 밀집 표현으로 바꿔주는 층을 말합니다.\n",
        "- 적당한 차운의 밀집 표현으로 바꿔준다면 딥려닝 모델의 학습에 큰 도움이 됩니다."
      ],
      "metadata": {
        "id": "_Wxo5lU1_eL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "   def __init__(self, num_embeddings):\n",
        "       super(LSTM, self).__init__()\n",
        "\n",
        "       # ❶ 밀집표현을 위한 임베딩층\n",
        "       # num_embeddings는 BOW 단어 개수를 의미합니다.\n",
        "       # embedding_dim은 밀집표현의 차원을 의미합니다\n",
        "       self.embed = nn.Embedding(\n",
        "           num_embeddings=num_embeddings, embedding_dim=16)\n",
        "\n",
        "       # LSTM을 5개층을 쌓음\n",
        "       self.lstm = nn.LSTM(\n",
        "           input_size=16,\n",
        "           hidden_size=64,\n",
        "           num_layers=5,\n",
        "           batch_first=True)\n",
        "\n",
        "       # 분류를 위한 MLP층\n",
        "       self.fc1 = nn.Linear(128, num_embeddings)\n",
        "       self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.embed(x)\n",
        "\n",
        "       # ❷ LSTM 모델의 예측값\n",
        "       # 전체 출력 값과 마지막 은닉 상태를 반환합니다\n",
        "       x, _ = self.lstm(x)\n",
        "       x = torch.reshape(x, (x.shape[0], -1))\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "k0Vnnr9S_c5N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습하기"
      ],
      "metadata": {
        "id": "L43D6fyMHInH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습을 진행할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset = TextGeneration()  # 데이터셋 정의\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)  # 모델 정의\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "   iterator = tqdm.tqdm(loader)\n",
        "   for data, label in iterator:\n",
        "       # 기울기 초기화\n",
        "       optim.zero_grad()\n",
        "\n",
        "       # 모델의 예측값\n",
        "       pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "\n",
        "       # 정답 레이블은 long 텐서로 반환해야 함\n",
        "       loss = nn.CrossEntropyLoss()(\n",
        "           pred, torch.tensor(label, dtype=torch.long).to(device))\n",
        "\n",
        "       # 오차 역전파\n",
        "       loss.backward()\n",
        "       optim.step()\n",
        "\n",
        "       iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shk9buyGHHov",
        "outputId": "bd115c43-41f4-4cb5-84c2-bb34f6d05959"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/104 [00:00<?, ?it/s]<ipython-input-28-a98fe878d097>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
            "<ipython-input-28-a98fe878d097>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred, torch.tensor(label, dtype=torch.long).to(device))\n",
            "epoch0 loss:7.412463188171387: 100%|██████████| 104/104 [00:01<00:00, 63.72it/s]\n",
            "epoch1 loss:6.948315143585205: 100%|██████████| 104/104 [00:01<00:00, 88.19it/s]\n",
            "epoch2 loss:6.293232440948486: 100%|██████████| 104/104 [00:01<00:00, 87.56it/s]\n",
            "epoch3 loss:5.873846054077148: 100%|██████████| 104/104 [00:01<00:00, 85.95it/s]\n",
            "epoch4 loss:5.645787715911865: 100%|██████████| 104/104 [00:01<00:00, 87.07it/s]\n",
            "epoch5 loss:5.641880512237549: 100%|██████████| 104/104 [00:01<00:00, 88.93it/s]\n",
            "epoch6 loss:5.648515701293945: 100%|██████████| 104/104 [00:01<00:00, 85.58it/s]\n",
            "epoch7 loss:5.721367835998535: 100%|██████████| 104/104 [00:01<00:00, 79.87it/s]\n",
            "epoch8 loss:6.301544666290283: 100%|██████████| 104/104 [00:01<00:00, 73.43it/s]\n",
            "epoch9 loss:6.1180524826049805: 100%|██████████| 104/104 [00:01<00:00, 79.34it/s]\n",
            "epoch10 loss:6.192314624786377: 100%|██████████| 104/104 [00:01<00:00, 74.15it/s]\n",
            "epoch11 loss:6.104078769683838: 100%|██████████| 104/104 [00:01<00:00, 86.79it/s]\n",
            "epoch12 loss:5.811516284942627: 100%|██████████| 104/104 [00:01<00:00, 88.60it/s]\n",
            "epoch13 loss:7.074960708618164: 100%|██████████| 104/104 [00:01<00:00, 87.77it/s]\n",
            "epoch14 loss:6.947862148284912: 100%|██████████| 104/104 [00:01<00:00, 88.38it/s]\n",
            "epoch15 loss:6.547462463378906: 100%|██████████| 104/104 [00:01<00:00, 88.57it/s]\n",
            "epoch16 loss:6.558929443359375: 100%|██████████| 104/104 [00:01<00:00, 87.88it/s]\n",
            "epoch17 loss:6.423494338989258: 100%|██████████| 104/104 [00:01<00:00, 88.34it/s]\n",
            "epoch18 loss:6.157176494598389: 100%|██████████| 104/104 [00:01<00:00, 87.39it/s]\n",
            "epoch19 loss:6.308128833770752: 100%|██████████| 104/104 [00:01<00:00, 80.39it/s]\n",
            "epoch20 loss:6.430638313293457: 100%|██████████| 104/104 [00:01<00:00, 81.23it/s]\n",
            "epoch21 loss:6.248127460479736: 100%|██████████| 104/104 [00:01<00:00, 79.55it/s]\n",
            "epoch22 loss:6.1603684425354: 100%|██████████| 104/104 [00:01<00:00, 77.40it/s]\n",
            "epoch23 loss:6.115574836730957: 100%|██████████| 104/104 [00:01<00:00, 78.87it/s]\n",
            "epoch24 loss:5.872981548309326: 100%|██████████| 104/104 [00:01<00:00, 87.20it/s]\n",
            "epoch25 loss:5.923373699188232: 100%|██████████| 104/104 [00:01<00:00, 86.64it/s]\n",
            "epoch26 loss:5.750574111938477: 100%|██████████| 104/104 [00:01<00:00, 86.51it/s]\n",
            "epoch27 loss:5.6556010246276855: 100%|██████████| 104/104 [00:01<00:00, 86.61it/s]\n",
            "epoch28 loss:5.534462928771973: 100%|██████████| 104/104 [00:01<00:00, 86.75it/s]\n",
            "epoch29 loss:5.43383264541626: 100%|██████████| 104/104 [00:01<00:00, 87.04it/s]\n",
            "epoch30 loss:5.315703868865967: 100%|██████████| 104/104 [00:01<00:00, 87.59it/s]\n",
            "epoch31 loss:5.250107288360596: 100%|██████████| 104/104 [00:01<00:00, 85.03it/s]\n",
            "epoch32 loss:5.078678607940674: 100%|██████████| 104/104 [00:01<00:00, 76.97it/s]\n",
            "epoch33 loss:5.062008380889893: 100%|██████████| 104/104 [00:01<00:00, 77.58it/s]\n",
            "epoch34 loss:4.895540714263916: 100%|██████████| 104/104 [00:01<00:00, 74.66it/s]\n",
            "epoch35 loss:4.852807998657227: 100%|██████████| 104/104 [00:01<00:00, 76.87it/s]\n",
            "epoch36 loss:4.798638343811035: 100%|██████████| 104/104 [00:01<00:00, 85.88it/s]\n",
            "epoch37 loss:5.032081127166748: 100%|██████████| 104/104 [00:01<00:00, 86.79it/s]\n",
            "epoch38 loss:4.980786323547363: 100%|██████████| 104/104 [00:01<00:00, 87.91it/s]\n",
            "epoch39 loss:5.077552318572998: 100%|██████████| 104/104 [00:01<00:00, 87.11it/s]\n",
            "epoch40 loss:4.862700939178467: 100%|██████████| 104/104 [00:01<00:00, 88.01it/s]\n",
            "epoch41 loss:4.915158271789551: 100%|██████████| 104/104 [00:01<00:00, 87.42it/s]\n",
            "epoch42 loss:4.914180278778076: 100%|██████████| 104/104 [00:01<00:00, 86.76it/s]\n",
            "epoch43 loss:4.848206520080566: 100%|██████████| 104/104 [00:01<00:00, 87.18it/s]\n",
            "epoch44 loss:4.670450210571289: 100%|██████████| 104/104 [00:01<00:00, 80.90it/s]\n",
            "epoch45 loss:4.533071517944336: 100%|██████████| 104/104 [00:01<00:00, 79.37it/s]\n",
            "epoch46 loss:4.491172790527344: 100%|██████████| 104/104 [00:01<00:00, 78.84it/s]\n",
            "epoch47 loss:4.551614761352539: 100%|██████████| 104/104 [00:01<00:00, 80.89it/s]\n",
            "epoch48 loss:4.531137466430664: 100%|██████████| 104/104 [00:01<00:00, 77.23it/s]\n",
            "epoch49 loss:4.596786022186279: 100%|██████████| 104/104 [00:01<00:00, 86.21it/s]\n",
            "epoch50 loss:4.571334362030029: 100%|██████████| 104/104 [00:01<00:00, 86.71it/s]\n",
            "epoch51 loss:4.563902378082275: 100%|██████████| 104/104 [00:01<00:00, 85.98it/s]\n",
            "epoch52 loss:4.563934326171875: 100%|██████████| 104/104 [00:01<00:00, 86.55it/s]\n",
            "epoch53 loss:4.46427059173584: 100%|██████████| 104/104 [00:01<00:00, 86.15it/s]\n",
            "epoch54 loss:4.388280868530273: 100%|██████████| 104/104 [00:01<00:00, 87.46it/s]\n",
            "epoch55 loss:4.398041725158691: 100%|██████████| 104/104 [00:01<00:00, 87.13it/s]\n",
            "epoch56 loss:4.459112644195557: 100%|██████████| 104/104 [00:01<00:00, 87.05it/s]\n",
            "epoch57 loss:4.585630416870117: 100%|██████████| 104/104 [00:01<00:00, 77.64it/s]\n",
            "epoch58 loss:4.21396541595459: 100%|██████████| 104/104 [00:01<00:00, 76.88it/s]\n",
            "epoch59 loss:4.039139747619629: 100%|██████████| 104/104 [00:01<00:00, 75.75it/s]\n",
            "epoch60 loss:3.9096992015838623: 100%|██████████| 104/104 [00:01<00:00, 75.24it/s]\n",
            "epoch61 loss:3.9096479415893555: 100%|██████████| 104/104 [00:01<00:00, 82.33it/s]\n",
            "epoch62 loss:3.8736448287963867: 100%|██████████| 104/104 [00:01<00:00, 84.88it/s]\n",
            "epoch63 loss:3.9082274436950684: 100%|██████████| 104/104 [00:01<00:00, 87.26it/s]\n",
            "epoch64 loss:3.8715643882751465: 100%|██████████| 104/104 [00:01<00:00, 87.00it/s]\n",
            "epoch65 loss:3.898754358291626: 100%|██████████| 104/104 [00:01<00:00, 86.40it/s]\n",
            "epoch66 loss:3.820328712463379: 100%|██████████| 104/104 [00:01<00:00, 86.57it/s]\n",
            "epoch67 loss:3.694333076477051: 100%|██████████| 104/104 [00:01<00:00, 86.40it/s]\n",
            "epoch68 loss:3.6778533458709717: 100%|██████████| 104/104 [00:01<00:00, 86.86it/s]\n",
            "epoch69 loss:3.4781494140625: 100%|██████████| 104/104 [00:01<00:00, 82.28it/s]\n",
            "epoch70 loss:3.4603850841522217: 100%|██████████| 104/104 [00:01<00:00, 76.81it/s]\n",
            "epoch71 loss:3.4208991527557373: 100%|██████████| 104/104 [00:01<00:00, 76.45it/s]\n",
            "epoch72 loss:3.5716612339019775: 100%|██████████| 104/104 [00:01<00:00, 76.66it/s]\n",
            "epoch73 loss:3.796877861022949: 100%|██████████| 104/104 [00:01<00:00, 77.77it/s]\n",
            "epoch74 loss:3.5621275901794434: 100%|██████████| 104/104 [00:01<00:00, 83.31it/s]\n",
            "epoch75 loss:3.539041042327881: 100%|██████████| 104/104 [00:01<00:00, 86.70it/s]\n",
            "epoch76 loss:3.675351142883301: 100%|██████████| 104/104 [00:01<00:00, 86.98it/s]\n",
            "epoch77 loss:3.5469512939453125: 100%|██████████| 104/104 [00:01<00:00, 88.23it/s]\n",
            "epoch78 loss:3.4486162662506104: 100%|██████████| 104/104 [00:01<00:00, 87.95it/s]\n",
            "epoch79 loss:3.3756113052368164: 100%|██████████| 104/104 [00:01<00:00, 85.84it/s]\n",
            "epoch80 loss:3.303558588027954: 100%|██████████| 104/104 [00:01<00:00, 87.12it/s]\n",
            "epoch81 loss:3.2208168506622314: 100%|██████████| 104/104 [00:01<00:00, 87.30it/s]\n",
            "epoch82 loss:3.262936592102051: 100%|██████████| 104/104 [00:01<00:00, 83.06it/s]\n",
            "epoch83 loss:3.2090795040130615: 100%|██████████| 104/104 [00:01<00:00, 83.62it/s]\n",
            "epoch84 loss:3.421168327331543: 100%|██████████| 104/104 [00:01<00:00, 79.50it/s]\n",
            "epoch85 loss:3.380129814147949: 100%|██████████| 104/104 [00:01<00:00, 79.99it/s]\n",
            "epoch86 loss:3.450446128845215: 100%|██████████| 104/104 [00:01<00:00, 84.40it/s]\n",
            "epoch87 loss:3.251528024673462: 100%|██████████| 104/104 [00:01<00:00, 80.02it/s]\n",
            "epoch88 loss:2.987760066986084: 100%|██████████| 104/104 [00:01<00:00, 86.41it/s]\n",
            "epoch89 loss:2.8960258960723877: 100%|██████████| 104/104 [00:01<00:00, 86.26it/s]\n",
            "epoch90 loss:3.2361762523651123: 100%|██████████| 104/104 [00:01<00:00, 86.23it/s]\n",
            "epoch91 loss:2.8857367038726807: 100%|██████████| 104/104 [00:01<00:00, 85.96it/s]\n",
            "epoch92 loss:2.8042304515838623: 100%|██████████| 104/104 [00:01<00:00, 86.92it/s]\n",
            "epoch93 loss:2.6514251232147217: 100%|██████████| 104/104 [00:01<00:00, 86.15it/s]\n",
            "epoch94 loss:2.646901845932007: 100%|██████████| 104/104 [00:01<00:00, 86.34it/s]\n",
            "epoch95 loss:2.592012882232666: 100%|██████████| 104/104 [00:01<00:00, 85.05it/s]\n",
            "epoch96 loss:2.714174509048462: 100%|██████████| 104/104 [00:01<00:00, 79.55it/s]\n",
            "epoch97 loss:2.6086151599884033: 100%|██████████| 104/104 [00:01<00:00, 74.74it/s]\n",
            "epoch98 loss:2.6322684288024902: 100%|██████████| 104/104 [00:01<00:00, 77.10it/s]\n",
            "epoch99 loss:2.4481911659240723: 100%|██████████| 104/104 [00:01<00:00, 78.31it/s]\n",
            "epoch100 loss:2.344611167907715: 100%|██████████| 104/104 [00:01<00:00, 80.72it/s]\n",
            "epoch101 loss:2.0717782974243164: 100%|██████████| 104/104 [00:01<00:00, 86.46it/s]\n",
            "epoch102 loss:2.2566754817962646: 100%|██████████| 104/104 [00:01<00:00, 85.72it/s]\n",
            "epoch103 loss:2.1445066928863525: 100%|██████████| 104/104 [00:01<00:00, 85.39it/s]\n",
            "epoch104 loss:2.235562562942505: 100%|██████████| 104/104 [00:01<00:00, 85.75it/s]\n",
            "epoch105 loss:2.0899717807769775: 100%|██████████| 104/104 [00:01<00:00, 85.57it/s]\n",
            "epoch106 loss:2.1046690940856934: 100%|██████████| 104/104 [00:01<00:00, 84.83it/s]\n",
            "epoch107 loss:2.184264659881592: 100%|██████████| 104/104 [00:01<00:00, 85.72it/s]\n",
            "epoch108 loss:2.0720643997192383: 100%|██████████| 104/104 [00:01<00:00, 84.59it/s]\n",
            "epoch109 loss:1.8698039054870605: 100%|██████████| 104/104 [00:01<00:00, 77.95it/s]\n",
            "epoch110 loss:2.029308795928955: 100%|██████████| 104/104 [00:01<00:00, 79.43it/s]\n",
            "epoch111 loss:2.0610551834106445: 100%|██████████| 104/104 [00:01<00:00, 77.60it/s]\n",
            "epoch112 loss:1.765368938446045: 100%|██████████| 104/104 [00:01<00:00, 73.10it/s]\n",
            "epoch113 loss:1.8524216413497925: 100%|██████████| 104/104 [00:01<00:00, 81.18it/s]\n",
            "epoch114 loss:1.8258862495422363: 100%|██████████| 104/104 [00:01<00:00, 83.80it/s]\n",
            "epoch115 loss:1.7472587823867798: 100%|██████████| 104/104 [00:01<00:00, 86.61it/s]\n",
            "epoch116 loss:1.90412175655365: 100%|██████████| 104/104 [00:01<00:00, 86.01it/s]\n",
            "epoch117 loss:1.9201794862747192: 100%|██████████| 104/104 [00:01<00:00, 86.18it/s]\n",
            "epoch118 loss:1.931455135345459: 100%|██████████| 104/104 [00:01<00:00, 86.61it/s]\n",
            "epoch119 loss:1.8025552034378052: 100%|██████████| 104/104 [00:01<00:00, 85.00it/s]\n",
            "epoch120 loss:1.8375011682510376: 100%|██████████| 104/104 [00:01<00:00, 86.15it/s]\n",
            "epoch121 loss:1.7273029088974: 100%|██████████| 104/104 [00:01<00:00, 80.96it/s]\n",
            "epoch122 loss:1.6137099266052246: 100%|██████████| 104/104 [00:01<00:00, 80.04it/s]\n",
            "epoch123 loss:1.7322697639465332: 100%|██████████| 104/104 [00:01<00:00, 76.12it/s]\n",
            "epoch124 loss:1.7259478569030762: 100%|██████████| 104/104 [00:01<00:00, 77.99it/s]\n",
            "epoch125 loss:1.5990447998046875: 100%|██████████| 104/104 [00:01<00:00, 75.51it/s]\n",
            "epoch126 loss:1.7211512327194214: 100%|██████████| 104/104 [00:01<00:00, 80.59it/s]\n",
            "epoch127 loss:1.4691262245178223: 100%|██████████| 104/104 [00:01<00:00, 84.93it/s]\n",
            "epoch128 loss:1.6946780681610107: 100%|██████████| 104/104 [00:01<00:00, 85.37it/s]\n",
            "epoch129 loss:1.489129900932312: 100%|██████████| 104/104 [00:01<00:00, 85.95it/s]\n",
            "epoch130 loss:1.5458534955978394: 100%|██████████| 104/104 [00:01<00:00, 84.84it/s]\n",
            "epoch131 loss:1.5458118915557861: 100%|██████████| 104/104 [00:01<00:00, 86.23it/s]\n",
            "epoch132 loss:1.3412587642669678: 100%|██████████| 104/104 [00:01<00:00, 86.05it/s]\n",
            "epoch133 loss:1.4287643432617188: 100%|██████████| 104/104 [00:01<00:00, 85.58it/s]\n",
            "epoch134 loss:1.4687130451202393: 100%|██████████| 104/104 [00:01<00:00, 80.02it/s]\n",
            "epoch135 loss:1.31918466091156: 100%|██████████| 104/104 [00:01<00:00, 78.76it/s]\n",
            "epoch136 loss:1.2904340028762817: 100%|██████████| 104/104 [00:01<00:00, 78.05it/s]\n",
            "epoch137 loss:1.046194314956665: 100%|██████████| 104/104 [00:01<00:00, 79.54it/s]\n",
            "epoch138 loss:1.1647471189498901: 100%|██████████| 104/104 [00:01<00:00, 82.55it/s]\n",
            "epoch139 loss:1.070672869682312: 100%|██████████| 104/104 [00:01<00:00, 78.36it/s]\n",
            "epoch140 loss:1.066749095916748: 100%|██████████| 104/104 [00:01<00:00, 86.18it/s]\n",
            "epoch141 loss:1.0648467540740967: 100%|██████████| 104/104 [00:01<00:00, 85.12it/s]\n",
            "epoch142 loss:1.0395023822784424: 100%|██████████| 104/104 [00:01<00:00, 84.46it/s]\n",
            "epoch143 loss:0.9680979251861572: 100%|██████████| 104/104 [00:01<00:00, 85.15it/s]\n",
            "epoch144 loss:0.9642536044120789: 100%|██████████| 104/104 [00:01<00:00, 61.63it/s]\n",
            "epoch145 loss:1.0748244524002075: 100%|██████████| 104/104 [00:01<00:00, 77.84it/s]\n",
            "epoch146 loss:0.9621776342391968: 100%|██████████| 104/104 [00:01<00:00, 80.00it/s]\n",
            "epoch147 loss:0.9567775726318359: 100%|██████████| 104/104 [00:01<00:00, 63.33it/s]\n",
            "epoch148 loss:1.0174365043640137: 100%|██████████| 104/104 [00:01<00:00, 63.65it/s]\n",
            "epoch149 loss:1.0165650844573975: 100%|██████████| 104/104 [00:01<00:00, 65.35it/s]\n",
            "epoch150 loss:1.1837385892868042: 100%|██████████| 104/104 [00:01<00:00, 54.76it/s]\n",
            "epoch151 loss:1.0219122171401978: 100%|██████████| 104/104 [00:01<00:00, 64.87it/s]\n",
            "epoch152 loss:0.7670316696166992: 100%|██████████| 104/104 [00:01<00:00, 74.14it/s]\n",
            "epoch153 loss:0.9953836798667908: 100%|██████████| 104/104 [00:01<00:00, 62.72it/s]\n",
            "epoch154 loss:0.6808256506919861: 100%|██████████| 104/104 [00:01<00:00, 71.38it/s]\n",
            "epoch155 loss:0.6797417998313904: 100%|██████████| 104/104 [00:01<00:00, 76.66it/s]\n",
            "epoch156 loss:1.1114251613616943: 100%|██████████| 104/104 [00:01<00:00, 85.37it/s]\n",
            "epoch157 loss:1.1701651811599731: 100%|██████████| 104/104 [00:01<00:00, 85.12it/s]\n",
            "epoch158 loss:1.146838665008545: 100%|██████████| 104/104 [00:01<00:00, 78.20it/s]\n",
            "epoch159 loss:0.4203964173793793: 100%|██████████| 104/104 [00:01<00:00, 72.03it/s]\n",
            "epoch160 loss:0.412242591381073: 100%|██████████| 104/104 [00:01<00:00, 79.95it/s]\n",
            "epoch161 loss:0.4550740122795105: 100%|██████████| 104/104 [00:01<00:00, 78.35it/s]\n",
            "epoch162 loss:0.6446068286895752: 100%|██████████| 104/104 [00:01<00:00, 83.11it/s]\n",
            "epoch163 loss:0.645997941493988: 100%|██████████| 104/104 [00:01<00:00, 72.72it/s]\n",
            "epoch164 loss:0.5804677605628967: 100%|██████████| 104/104 [00:01<00:00, 83.95it/s]\n",
            "epoch165 loss:0.4941495656967163: 100%|██████████| 104/104 [00:01<00:00, 84.91it/s]\n",
            "epoch166 loss:0.7698964476585388: 100%|██████████| 104/104 [00:01<00:00, 84.23it/s]\n",
            "epoch167 loss:0.45825472474098206: 100%|██████████| 104/104 [00:01<00:00, 84.69it/s]\n",
            "epoch168 loss:0.4814826250076294: 100%|██████████| 104/104 [00:01<00:00, 82.95it/s]\n",
            "epoch169 loss:0.4691462814807892: 100%|██████████| 104/104 [00:01<00:00, 85.33it/s]\n",
            "epoch170 loss:0.37453994154930115: 100%|██████████| 104/104 [00:01<00:00, 85.09it/s]\n",
            "epoch171 loss:0.27936840057373047: 100%|██████████| 104/104 [00:01<00:00, 84.31it/s]\n",
            "epoch172 loss:0.42700326442718506: 100%|██████████| 104/104 [00:01<00:00, 76.49it/s]\n",
            "epoch173 loss:0.37002354860305786: 100%|██████████| 104/104 [00:01<00:00, 82.10it/s]\n",
            "epoch174 loss:0.3495941162109375: 100%|██████████| 104/104 [00:01<00:00, 74.40it/s]\n",
            "epoch175 loss:0.3452681601047516: 100%|██████████| 104/104 [00:01<00:00, 74.92it/s]\n",
            "epoch176 loss:0.45762017369270325: 100%|██████████| 104/104 [00:01<00:00, 75.95it/s]\n",
            "epoch177 loss:0.26698020100593567: 100%|██████████| 104/104 [00:01<00:00, 82.39it/s]\n",
            "epoch178 loss:0.2924150228500366: 100%|██████████| 104/104 [00:01<00:00, 85.39it/s]\n",
            "epoch179 loss:0.38435468077659607: 100%|██████████| 104/104 [00:01<00:00, 85.45it/s]\n",
            "epoch180 loss:0.26912543177604675: 100%|██████████| 104/104 [00:01<00:00, 85.27it/s]\n",
            "epoch181 loss:0.5003976225852966: 100%|██████████| 104/104 [00:01<00:00, 84.84it/s]\n",
            "epoch182 loss:0.25742238759994507: 100%|██████████| 104/104 [00:01<00:00, 85.18it/s]\n",
            "epoch183 loss:0.25497716665267944: 100%|██████████| 104/104 [00:01<00:00, 85.57it/s]\n",
            "epoch184 loss:0.22038091719150543: 100%|██████████| 104/104 [00:01<00:00, 86.01it/s]\n",
            "epoch185 loss:0.2251240611076355: 100%|██████████| 104/104 [00:01<00:00, 81.60it/s]\n",
            "epoch186 loss:0.5621270537376404: 100%|██████████| 104/104 [00:01<00:00, 76.36it/s]\n",
            "epoch187 loss:0.46861371397972107: 100%|██████████| 104/104 [00:01<00:00, 77.57it/s]\n",
            "epoch188 loss:0.3685464859008789: 100%|██████████| 104/104 [00:01<00:00, 74.25it/s]\n",
            "epoch189 loss:0.2510432004928589: 100%|██████████| 104/104 [00:01<00:00, 75.62it/s]\n",
            "epoch190 loss:0.21374449133872986: 100%|██████████| 104/104 [00:01<00:00, 80.34it/s]\n",
            "epoch191 loss:0.2867700159549713: 100%|██████████| 104/104 [00:01<00:00, 84.24it/s]\n",
            "epoch192 loss:0.28751346468925476: 100%|██████████| 104/104 [00:01<00:00, 84.77it/s]\n",
            "epoch193 loss:0.3322649300098419: 100%|██████████| 104/104 [00:01<00:00, 85.50it/s]\n",
            "epoch194 loss:0.26325953006744385: 100%|██████████| 104/104 [00:01<00:00, 83.79it/s]\n",
            "epoch195 loss:0.1394094079732895: 100%|██████████| 104/104 [00:01<00:00, 84.98it/s]\n",
            "epoch196 loss:0.22644542157649994: 100%|██████████| 104/104 [00:01<00:00, 84.48it/s]\n",
            "epoch197 loss:0.15991492569446564: 100%|██████████| 104/104 [00:01<00:00, 84.60it/s]\n",
            "epoch198 loss:0.15195845067501068: 100%|██████████| 104/104 [00:01<00:00, 77.14it/s]\n",
            "epoch199 loss:0.6702495217323303: 100%|██████████| 104/104 [00:01<00:00, 77.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor(\n",
        "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "           # ❶\n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           # argmax(A) : 텐서 A의 최댓값이 들어 있는 요소의 번호를 반환합니다.\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSdY-H0nH0Cm",
        "outputId": "419aefa5-ed5f-41d4-9b17-ea4b1076a296"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: finding an \n",
            "predicted sentence: finding an tries money’ another left in iraq picture unspeakably nod survey \n"
          ]
        }
      ]
    }
  ]
}