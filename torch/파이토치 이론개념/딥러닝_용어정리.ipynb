{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [용어정리] 딥러닝 기본 개념 정리\n",
    "\n",
    "\n",
    "1. 인공 뉴런(퍼셉트론)\n",
    "\n",
    "- 입력값과 가중치, 편향을 이용해 출력값을 내는 수학적 모델\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. 단층 인공 신경망\n",
    "\n",
    "- 퍼셉트론을 하나만 사용하는 인공 신경망\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. 다층 인공 신경망\n",
    "\n",
    "- 퍼셉트론을 여러 개 사용하는 인공 신경망\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 입력층, 출력층, 은닉층\n",
    "\n",
    "- 입력값을 표현하는 입력층, 신경망의 출력을 계산하는 출력층, 입력층 이후부터 출력증 전까지는 은닉층\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. 가중치\n",
    "\n",
    "- 입력의 중요도를 나타내고 편향은 활성화의 경계가 원점으로부터 얼마나 이동할지를 결정\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. 활성화 함수\n",
    "\n",
    "- 해당 뉴런의 출력을 다음 뉴런으로 넘길지를 결정\n",
    "\n",
    "시그모이드 함수는 뉴런의 출력 값을 0과 1 사이로 고정\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7. 손실 함수\n",
    "\n",
    "- 정답과 신경망의 예측의 차이를 나타내는 함수\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. 경사 하강법\n",
    "\n",
    "- 손실을 가중치에 대해 미분한 다음, 기울기의 반대 방향으로 학습률만큼 이동시키는 알고리즘\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9. 오차 역전파\n",
    "\n",
    "- 올바른 가중치를 찾기 위해 오차를 출력층으로부터 입력층까지 전파하는 방식\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10. 오버피팅\n",
    "\n",
    "- 과적합이라고도 함. 학습에 사용한 데이터에 최적화되게 학습되어 다른 데이터에 대한 예측 성능이 떨어지는 경우\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11. 기울기 소실\n",
    "\n",
    "- 출력층으로부터 멀어질수록 역전파되는 오차가 0에 가까워지는 현상\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "### 손실함수의 용도와 설명\n",
    "\n",
    "\n",
    "**평균 제곱 오차(Mean Squared Error, MSE)**\n",
    "\n",
    "용도 : 회귀\n",
    "\n",
    "설명 : 정답과 예측값의 차의 제곱의 평균값. 1보다 작은 오차는 더 작게, 1보다 큰 오차는 더 크게 키우는 특성\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**크로스 엔트로피 오차(Cross Entropy Error, CE)**\n",
    "\n",
    "용도 : 이진분류, 다중분류\n",
    "\n",
    "설명 : 두 확률 분포의 차이를 구하는 함수. 분류 문제에서는 인공 신경망의 출력이 확률 분포이므로 확률 분포의 차를 구하는 함수가 필요함. 크로스 엔트로피는 정답값이 확률과 모델이 예측한 확률에 로그를 취한 값을 곱해서 구함\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**평균 절대 오차(Mean Average Error, MAE)**\n",
    "\n",
    "용도 : 회귀\n",
    "\n",
    "설명 : 정답과 예측값의 차이의 절대값의 평균값. 1보다 작은 오차도 놓치지 않는 꼼꼼함을 갖고 있지만, 오차 크기가 아니라 부호에만 의존하기 때문에 작은 오차라도 기울기가 커질 수 있으므로 학습이 불안정\n",
    "\n",
    "<br>\n",
    "\n",
    "**평균 제곱근 오차(Root Mean Squared Error, RMSE)**\n",
    "\n",
    "용도 : 회귀\n",
    "\n",
    "설명 : MSE의 제곱근. 큰 오차에 대한 민감도를 줄여줌\n",
    "\n",
    "<br>\n",
    "-----------------------------------\n",
    "\n",
    "### 경사하강법\n",
    "\n",
    "- 함수의 기울기(경사)를 구하고 경사의 반대 방향으로 계속 이동시켜 최솟값에 이를 때까지 반복시키는 학습 방법\n",
    "\n",
    "기울기의 반대 방향으로, 즉 기울기 값에 -1을 곱한 값만큼 변수의 값을 이동시키면서 최솟값에 조금씩 다가가는 알고리즘\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**오차 역전파**\n",
    "\n",
    "- 정답과 신경망이 예측한 값과의 오차를 최소화하는 가중치를 찾는 알고리즘.\n",
    "\n",
    "미분의 연쇄 법칙을 이용해 출력층에 가까운 가중치부터 수정\n",
    "\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "### 활성화 함수\n",
    "\n",
    "\n",
    "> 시그모이드(Sigmoid)\n",
    "\n",
    "설명 : 실수 전체의 입력값을 0과 1 사이로 제한하는 함수.\n",
    "\n",
    "무한한 실수를 0과 1사이로 일대일 대응시킴. \n",
    "\n",
    "하나의 클래스에 속할 확률이 p라면 반대 클래스에 속할 확률은 1-p가 되므로 이진분류에 사용\n",
    "\n",
    "<br>\n",
    "\n",
    "> ReLU(Rectified Linear Unit)\n",
    "\n",
    "설명 : 0보다 작은 값은 0으로, 0보다 크거나 같은 값은 입력값을 그대로 출력하는 함수. 주로 은닉층의 활성화에 이용\n",
    "\n",
    "<br>\n",
    "\n",
    "> 소프트맥스(Softmax)\n",
    "\n",
    "설명 : k개의 숫자를 입력받아 k개의 요소를 갖는 확률 분포로 변환하는 함수.\n",
    "\n",
    "여러가지 출력값 즉, 여러 클래스에 속할 확률을 나타내므로 다중 분류에 사용"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
