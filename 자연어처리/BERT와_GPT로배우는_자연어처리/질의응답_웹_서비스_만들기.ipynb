{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "iBfvkvQ0MVDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ObHizD0MQx3"
      },
      "outputs": [],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 구글 드라이브 연동하기\n",
        "모델 체크포인트 등을 저장해 둘 구글 드라이브를 연결합니다. 자신의 구글 계정에 적용됩니다."
      ],
      "metadata": {
        "id": "trry7vF0MW0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "qZtCOqy2MXl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 각종 설정\n",
        "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다."
      ],
      "metadata": {
        "id": "LAiZa1UPMZlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ratsnlp.nlpbook.qa import QADeployArguments\n",
        "args = QADeployArguments(\n",
        "    pretrained_model_name=\"beomi/kcbert-base\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-qa\",\n",
        "    max_seq_length=128,\n",
        "    max_query_length=32,\n",
        ")"
      ],
      "metadata": {
        "id": "RB73xgKqMafF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 로딩\n",
        "파인튜닝을 마친 모델과 토크나이저를 읽어 들입니다."
      ],
      "metadata": {
        "id": "NNE5R_1bMbb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertConfig, BertForQuestionAnswering\n",
        "fine_tuned_model_ckpt = torch.load(\n",
        "    args.downstream_model_checkpoint_fpath,\n",
        "    map_location=torch.device(\"cpu\")\n",
        ")\n",
        "pretrained_model_config = BertConfig.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        ")\n",
        "model = BertForQuestionAnswering(pretrained_model_config)\n",
        "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "NLyvL8klMcF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "metadata": {
        "id": "5r2quednMfrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인퍼런스 함수 선언\n",
        "인퍼런스 함수를 선언합니다.\n",
        "\n",
        "인퍼런스 함수는 질문과 지문을 입력받아 토큰화를 수행한뒤,\n",
        "input_ids, attention_mask, token_type_ids를 만듭니다.\n",
        "이들 입력값을 파이토치 텐서 자료형으로 변환한 뒤 모델에 입력합니다.\n",
        "모델 출력값은 소프트맥스 함수 적용 이전의 로짓 형태입니다.\n",
        "\n",
        "마지막으로 모델 출력을 약간 후처리해서\n",
        "정답 시작 로직의 최댓값 위치부터 정답 끝 로짓의 최대값 위치의 토큰들을 이어붙여\n",
        "pred_text로 만듭니다."
      ],
      "metadata": {
        "id": "AiS4XxtEMhHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_fn(question, context):\n",
        "    if question and context:\n",
        "      # 질문을 토큰화하고 인덱싱하되\n",
        "      # max_query_length보다 길면 이에 맞게 자르기\n",
        "        truncated_query = tokenizer.encode(\n",
        "            question,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=args.max_query_length\n",
        "       )\n",
        "        # 앞서 처리한 질문을 지문과 함께 토큰화하고 인덱싱하되\n",
        "        # 전체 길이가 max_seq_length보다 길면 지문 자르기\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text=truncated_query,\n",
        "            text_pair=context,\n",
        "            truncation=\"only_second\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=args.max_seq_length,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**{k: torch.tensor([v]) for k, v in inputs.items()})\n",
        "            # 정답의 시작 위치와 관련된 로짓에서 가장 큰 값이 가리키는 토큰 위치를 알아내기\n",
        "            start_pred = outputs.start_logits.argmax(dim=-1).item()\n",
        "            # 정답의 끝 위치와 관련된 로짓에서 가장 큰 값이 가리키는 토큰 위치를 알아내기\n",
        "            end_pred = outputs.end_logits.argmax(dim=-1).item()\n",
        "            # 정답 시작부터 끝까지의 토큰들을 이어붙여 정답 만들기\n",
        "            pred_text = tokenizer.decode(inputs['input_ids'][start_pred:end_pred+1])\n",
        "    else:\n",
        "        pred_text = \"\"\n",
        "    return {\n",
        "        'question': question,\n",
        "        'context': context,\n",
        "        'answer': pred_text,\n",
        "    }"
      ],
      "metadata": {
        "id": "7wmq72yqNyMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 웹서비스 만들기 준비\n",
        "\n",
        "`ngrok`은 코랩 로컬에서 실행 중인 웹서비스를 안전하게 외부에서 접근 가능하도록 해주는 도구입니다. `ngrok`을 실행하려면 [회원가입](https://dashboard.ngrok.com/signup) 후 [로그인](https://dashboard.ngrok.com/login)을 한 뒤 [이곳](https://dashboard.ngrok.com/get-started/your-authtoken)에 접속해 인증 토큰(authtoken)을 확인해야 합니다. 예를 들어 확인된 `authtoken`이 `test111`이라면 다음과 같이 실행합니다.\n",
        "\n",
        "```bash\n",
        "!mkdir /root/.ngrok2 && echo \"authtoken: test111\" > /root/.ngrok2/ngrok.yml\n",
        "```"
      ],
      "metadata": {
        "id": "QNsFlnnsOlPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.ngrok2 && echo \"authtoken: {이곳에 확인된 인증 토큰을 입력하세요}\" > /root/.ngrok2/ngrok.yml"
      ],
      "metadata": {
        "id": "N4y2Uuc2Oluo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 웹서비스 개시\n",
        "아래처럼 실행해 인퍼런스 함수를 웹서비스로 만듭니다."
      ],
      "metadata": {
        "id": "ziPZP9kyOncr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ratsnlp.nlpbook.qa import get_web_service_app\n",
        "app = get_web_service_app(inference_fn)\n",
        "app.run()"
      ],
      "metadata": {
        "id": "Fzpxq-oROoi1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}