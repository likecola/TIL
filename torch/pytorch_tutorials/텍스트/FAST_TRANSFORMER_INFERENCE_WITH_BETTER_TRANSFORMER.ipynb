{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load torchtext and initialize XLM-R model\n",
        "- 이 코드는 PyTorch와 torchtext 라이브러리를 사용하여 XLM-R (Cross-lingual Language Model - RoBERTa) 모델을 로드하고 초기화한 다음, CPU 및 GPU에서의 성능을 측정하는 코드입니다"
      ],
      "metadata": {
        "id": "cJ5Q8SkFGB24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bFoCLU1Fx-N",
        "outputId": "5cca96fe-fa78-4a17-d96e-7883d0b71213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/text/xlmr.large.encoder.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.large.encoder.pt\n",
            "100%|██████████| 2.08G/2.08G [00:08<00:00, 250MB/s]\n",
            "100%|██████████| 5.07M/5.07M [00:00<00:00, 73.0MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/text/xlmr.vocab.pt\" to /root/.cache/torch/hub/checkpoints/xlmr.vocab.pt\n",
            "100%|██████████| 4.85M/4.85M [00:00<00:00, 69.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "\n",
        "from torchtext.models import RobertaClassificationHead\n",
        "from torchtext.functional import to_tensor\n",
        "\n",
        "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
        "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
        "model = xlmr_large.get_model(head=classifier_head)\n",
        "\n",
        "# 추론 모드로 모델 전환 (Better Transformer를 사용하지 않아도 런타임 감소, 특히 GPU 실행에서 필요)\n",
        "model.eval()\n",
        "\n",
        "# 입력 변환을 정의합니다.\n",
        "transform = xlmr_large.transform()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Information"
      ],
      "metadata": {
        "id": "9pDGjp9LGxlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "cpu = platform.processor()\n",
        "gpu = torch.cuda.get_device_name(DEVICE)\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"CPU type: {cpu}\")\n",
        "print(f\"GPU type: {gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyLyyb-FGvHh",
        "outputId": "c7a6577f-ec6b-42c7-f3c0-63d12306d55d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.1.0+cu118\n",
            "torch cuda available: True\n",
            "CPU type: x86_64\n",
            "GPU type: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 기본 희소성 지원 설정을 확인하세요.\n",
        "\n",
        "희소성 지원은 트랜스포머가 입력에서 패딩을 건너뛸 수 있게 합니다."
      ],
      "metadata": {
        "id": "7S-ORfYvHnZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10vNIAGOHLZh",
        "outputId": "7bac172a-eee5-4596-e78c-90e6e709b4b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark setup"
      ],
      "metadata": {
        "id": "fjxIM0yRHy1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define inputs"
      ],
      "metadata": {
        "id": "dm6I4WVeH1xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\"\n",
        "]\n",
        "big_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\",\n",
        "               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "if you still try to defend the infamies and horrors perpetrated by\n",
        "that Antichrist- I really believe he is Antichrist- I will have\n",
        "nothing more to do with you and you are no longer my friend, no longer\n",
        "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
        "I have frightened you- sit down and tell me all the news.`\n",
        "\n",
        "It was in July, 1805, and the speaker was the well-known Anna\n",
        "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
        "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
        "of high rank and importance, who was the first to arrive at her\n",
        "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
        "she said, suffering from la grippe; grippe being then a new word in\n",
        "St. Petersburg, used only by the elite.\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "3Fb287O_HusA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 작은 입력 집합 또는 큰 입력 집합을 선택하세요.\n",
        "아래의 input_batch에 할당을 수정하여 small_input_batch 또는 big_input_batch   중하나를 선택하거나, 자체 입력을 대체하세요."
      ],
      "metadata": {
        "id": "YJdiL4aNIeVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch=big_input_batch\n",
        "\n",
        "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
        "output = model(model_input)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCHykMorIJVx",
        "outputId": "b31e6ce7-b931-4d58-ba70-9c1c107b3a51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 측정을 위한 반복 횟수"
      ],
      "metadata": {
        "id": "AiJAxxZPIyrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ITERATIONS=10"
      ],
      "metadata": {
        "id": "AtAtZwSuIsMC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU 성능 측정: 느린 경로 및 빠른 경로, BT 희소성 없이 측정\n",
        "희소성 지원으로 인해 트랜스포머는 입력에서 패딩을 건너뛸 수 있습니다."
      ],
      "metadata": {
        "id": "2q4QKYvcJQpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU 성능: BT 희소성 없이 측정"
      ],
      "metadata": {
        "id": "9LH7byTqJWzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = False"
      ],
      "metadata": {
        "id": "fsnxT22EI7AQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v85NO_4MJdDB",
        "outputId": "30b86762-837e-441a-bb51-9ce1b490cda7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                 aten::addmm        63.96%       52.404s        65.05%       53.294s      72.019ms           740  \n",
            "                                    aten::mm        21.81%       17.873s        21.81%       17.873s      74.472ms           240  \n",
            "                                   aten::bmm         5.33%        4.368s         5.33%        4.368s       9.100ms           480  \n",
            "                              aten::_softmax         2.39%        1.957s         2.39%        1.957s       8.154ms           240  \n",
            "                                 aten::copy_         2.13%        1.743s         2.13%        1.743s     795.978us          2190  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 81.931s\n",
            "\n",
            "fast path:\n",
            "==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::addmm        31.59%       10.998s        31.75%       11.052s      22.104ms           500  \n",
            "                    aten::_addmm_activation        24.84%        8.647s        26.44%        9.206s      38.359ms           240  \n",
            "                                   aten::mm        18.88%        6.572s        18.88%        6.572s      27.382ms           240  \n",
            "                                  aten::bmm         9.37%        3.263s         9.37%        3.263s       6.798ms           480  \n",
            "          aten::_transform_bias_rescale_qkv         4.45%        1.550s         6.78%        2.361s       9.840ms           240  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 34.814s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU 성능: BT 희소성과 함께 측정"
      ],
      "metadata": {
        "id": "dpXBWXaKKNQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ],
      "metadata": {
        "id": "XoeThsnOKILh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "897JDNkzKWdB",
        "outputId": "ca71d3dd-a595-45d8-cbd3-0de5a725b718"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                 aten::addmm        65.76%       50.190s        66.41%       50.692s      68.502ms           740  \n",
            "                                    aten::mm        21.92%       16.734s        21.92%       16.734s      69.725ms           240  \n",
            "                                   aten::bmm         5.08%        3.878s         5.08%        3.878s       8.079ms           480  \n",
            "                              aten::_softmax         1.98%        1.511s         1.98%        1.511s       6.295ms           240  \n",
            "                                  aten::gelu         1.71%        1.308s         1.71%        1.308s       5.449ms           240  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 76.326s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::addmm        32.01%       10.239s        32.18%       10.295s      20.591ms           500  \n",
            "                    aten::_addmm_activation        24.58%        7.862s        26.20%        8.381s      34.921ms           240  \n",
            "                                   aten::mm        18.52%        5.926s        18.52%        5.926s      24.690ms           240  \n",
            "                                  aten::bmm         9.49%        3.035s         9.49%        3.035s       6.322ms           480  \n",
            "          aten::_transform_bias_rescale_qkv         4.47%        1.429s         6.81%        2.178s       9.074ms           240  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 31.990s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 장치(DEVICE) 성능 측정: 느린 경로 및 빠른 경로, 희소성 없이 및 희소성과 함께 측정\n",
        "Better Transformer의 빠른 경로 실행이 GPU에서 성능 이점을 제공하려면 런타임이 GPU를 활성화해야 합니다. Google Colab 메뉴에서 \"Runtime > Change Runtime Type\"을 통해 런타임 유형을 확인하고 변경할 수 있습니다."
      ],
      "metadata": {
        "id": "jgMrFOYKKh-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "model_input = model_input.to(DEVICE)"
      ],
      "metadata": {
        "id": "CWlphZA_KbKR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 장치(DEVICE) 성능: Better Transformer 희소성 없이 측정"
      ],
      "metadata": {
        "id": "W8eBrK6UKxJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor=False"
      ],
      "metadata": {
        "id": "XRi5Y-oxKr3x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WohOmyvcLPox",
        "outputId": "41398f16-ec1c-46ea-9fbf-3526c7d70945"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                               aten::mm         2.68%     112.138ms        74.27%        3.103s      12.929ms        3.354s        73.67%        3.354s      13.974ms           240  \n",
            "                                            aten::addmm         1.04%      43.329ms         1.63%      68.114ms      92.046us     755.467ms        16.59%     755.467ms       1.021ms           740  \n",
            "                     aten::_efficient_attention_forward         0.34%      14.238ms         0.47%      19.540ms      81.417us      94.578ms         2.08%      96.556ms     402.317us           240  \n",
            "                                            aten::copy_         0.39%      16.102ms         0.59%      24.647ms      20.369us      55.369ms         1.22%      55.369ms      45.760us          1210  \n",
            "                                aten::native_layer_norm         1.11%      46.312ms         1.40%      58.611ms     119.614us      26.271ms         0.58%      31.270ms      63.816us           490  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 4.178s\n",
            "Self CUDA time total: 4.553s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         2.75%      31.313ms         3.97%      45.264ms      61.168us     256.557ms        21.79%     256.557ms     346.699us           740  \n",
            "                                aten::_addmm_activation         1.14%      12.951ms         1.48%      16.904ms      70.433us     140.441ms        11.93%     140.441ms     585.171us           240  \n",
            "                                            aten::clone        12.99%     148.195ms        14.48%     165.134ms      21.502us      82.488ms         7.01%     134.683ms      17.537us          7680  \n",
            "                                            aten::empty         1.34%      15.324ms         1.34%      15.324ms       0.844us      66.996ms         5.69%      66.996ms       3.691us         18150  \n",
            "                                        aten::transpose        10.49%     119.644ms        22.30%     254.362ms      69.880us      65.017ms         5.52%     213.361ms      58.616us          3640  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.140s\n",
            "Self CUDA time total: 1.177s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 장치(DEVICE) 성능: Better Transformer 희소성과 함께 측정"
      ],
      "metadata": {
        "id": "G2kNssQ2LbVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ],
      "metadata": {
        "id": "V4kiRwi2LQ0B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model_input = model_input.to(DEVICE)\n",
        "\n",
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TcDlfopLiuh",
        "outputId": "baf6b38e-a776-4408-dfa5-ea9646b0444c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         4.10%      38.937ms         5.27%      49.998ms      67.565us     770.219ms        55.07%     770.219ms       1.041ms           740  \n",
            "                                               aten::mm         0.97%       9.166ms         1.29%      12.244ms      51.017us     268.050ms        19.17%     268.050ms       1.117ms           240  \n",
            "                     aten::_efficient_attention_forward         1.29%      12.247ms         1.85%      17.518ms      72.992us      93.513ms         6.69%      95.465ms     397.771us           240  \n",
            "                                            aten::copy_         1.49%      14.111ms         2.38%      22.569ms      18.652us      53.223ms         3.81%      53.223ms      43.986us          1210  \n",
            "                                             aten::gelu         0.53%       5.071ms         0.70%       6.643ms      27.679us      24.431ms         1.75%      24.431ms     101.796us           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 949.004ms\n",
            "Self CUDA time total: 1.399s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         2.79%      31.019ms         3.98%      44.190ms      59.716us     257.401ms        22.41%     257.401ms     347.839us           740  \n",
            "                                aten::_addmm_activation         0.86%       9.569ms         1.20%      13.375ms      55.729us     139.727ms        12.16%     139.727ms     582.196us           240  \n",
            "                                            aten::clone        13.54%     150.410ms        15.10%     167.741ms      21.841us      82.005ms         7.14%     135.088ms      17.590us          7680  \n",
            "                                            aten::empty         1.50%      16.715ms         1.50%      16.715ms       0.921us      67.002ms         5.83%      67.002ms       3.692us         18150  \n",
            "                                        aten::transpose        10.41%     115.643ms        22.76%     252.867ms      69.469us      66.797ms         5.81%     209.944ms      57.677us          3640  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.111s\n",
            "Self CUDA time total: 1.149s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lii6bfAkLjwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}