{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# torchtext 라이브러리로 텍스트 분류하기\n",
        "\n",
        "**번역**: [김강민](https://github.com/gangsss) , [김진현](https://github.com/lewha0)\n",
        "\n",
        "이 튜토리얼에서는 torchtext 라이브러리를 사용하여 어떻게 텍스트 분류 분석을 위한 데이터셋을 만드는지를 살펴보겠습니다.\n",
        "다음과 같은 내용들을 알게 됩니다:\n",
        "\n",
        "   - 반복자(iterator)로 가공되지 않은 데이터(raw data)에 접근하기\n",
        "   - 가공되지 않은 텍스트 문장들을 모델 학습에 사용할 수 있는 ``torch.Tensor`` 로 변환하는 데이터 처리 파이프라인 만들기\n",
        "   - [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)_ 를 사용하여 데이터를 섞고 반복하기(shuffle and iterate)\n"
      ],
      "metadata": {
        "id": "isZYver91GQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기초 데이터셋 반복자(raw data iterator)에 접근하기\n",
        "\n",
        "torchtext 라이브러리는 가공되지 않은 텍스트 문장들을 만드는(yield) 몇 가지 기초 데이터셋 반복자(raw dataset iterator)를 제공합니다.\n",
        "예를 들어, ``AG_NEWS`` 데이터셋 반복자는 레이블(label)과 문장의 튜플(tuple) 형태로 가공되지 않은 데이터를 만듭니다.\n",
        "\n",
        "torchtext 데이터셋에 접근하기 전에, https://github.com/pytorch/data 을 참고하여 torchdata를\n",
        "설치하시기 바랍니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Glo8Zxgd1INx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker>=2.0.0\n"
      ],
      "metadata": {
        "id": "VSPriVRQ1oK6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oznQYx2k1AnP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = iter(AG_NEWS(split='train'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "::\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (3, \"Fears for T N pension after talks Unions representing workers at Turner\n",
        "    Newall say they are 'disappointed' after talks with stricken parent firm Federal\n",
        "    Mogul.\")\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (4, \"The Race is On: Second Private Team Sets Launch Date for Human\n",
        "    Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of\n",
        "    rocketeers competing for the  #36;10 million Ansari X Prize, a contest\n",
        "    for\\\\privately funded suborbital space flight, has officially announced\n",
        "    the first\\\\launch date for its manned rocket.\")\n",
        "\n",
        "    next(train_iter)\n",
        "    >>> (4, 'Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded\n",
        "    by a chemistry researcher at the University of Louisville won a grant to develop\n",
        "    a method of producing better peptides, which are short chains of amino acids, the\n",
        "    building blocks of proteins.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZLTSdcs1gwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 처리 파이프라인 준비하기\n",
        "\n",
        "어휘집(vocab), 단어 벡터(word vector), 토크나이저(tokenizer)를 포함하여 torchtext 라이브러리의 가장 기본적인 구성요소를 재검토했습니다.\n",
        "이들은 가공되지 않은 텍스트 문자열에 대한 기본적인 데이터 처리 빌딩 블록(data processing building block)입니다.\n",
        "\n",
        "다음은 토크나이저 및 어휘집을 사용한 일반적인 NLP 데이터 처리의 예입니다.\n",
        "첫번째 단계는 가공되지 않은 학습 데이터셋으로 어휘집을 만드는 것입니다.\n",
        "여기에서는 토큰의 목록 또는 반복자를 받는 내장(built-in) 팩토리 함수(factory function) `build_vocab_from_iterator` 를 사용합니다.\n",
        "사용자는 어휘집에 추가할 특수 기호(special symbol) 같은 것들을 전달할 수도 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "gBQvdJa-1imI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "b_aubRZq1fHa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어휘집 블록(vocabulary block)은 토큰 목록을 정수로 변환합니다.\n",
        "\n",
        "::\n",
        "\n",
        "    vocab(['here', 'is', 'an', 'example'])\n",
        "    >>> [475, 21, 30, 5297]\n",
        "\n",
        "토크나이저와 어휘집을 갖춘 텍스트 처리 파이프라인을 준비합니다.\n",
        "텍스트 파이프라인과 레이블(label) 파이프라인은 데이터셋 반복자로부터 얻어온 가공되지 않은 문장 데이터를 처리하기 위해 사용됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lW7mLQ6i5L8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "4ek3h1ei22Hu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 파이프라인은 어휘집에 정의된 룩업 테이블(순람표; lookup table)에 기반하여 텍스트 문장을 정수 목록으로 변환합니다.\n",
        "레이블(label) 파이프라인은 레이블을 정수로 변환합니다. 예를 들어,\n",
        "\n",
        "::\n",
        "\n",
        "    text_pipeline('here is the an example')\n",
        "    >>> [475, 21, 2, 30, 5297]\n",
        "    label_pipeline('10')\n",
        "    >>> 9\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zvTwcFsg5bkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 배치(batch)와 반복자 생성하기\n",
        "\n",
        "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)_ 를\n",
        "권장합니다. (튜토리얼은 [여기](https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html)_ 있습니다.)\n",
        "이는 ``getitem()`` 과 ``len()`` 프로토콜을 구현한 맵 형태(map-style)의 데이터셋으로 동작하며, 맵(map)처럼 인덱스/키로 데이터 샘플을 얻어옵니다.\n",
        "또한, 셔플(shuffle) 인자를 ``False`` 로 설정하면 순회 가능한(iterable) 데이터셋처럼 동작합니다.\n",
        "\n",
        "모델로 보내기 전, ``collate_fn`` 함수는 ``DataLoader`` 로부터 생성된 샘플 배치로 동작합니다.\n",
        "``collate_fn`` 의 입력은 ``DataLoader`` 에 배치 크기(batch size)가 있는 배치(batch) 데이터이며,\n",
        "``collate_fn`` 은 이를 미리 선언된 데이터 처리 파이프라인에 따라 처리합니다.\n",
        "``collate_fn`` 이 최상위 수준으로 정의(top level def)되었는지 확인합니다. 이렇게 하면 모든 워커에서 이 함수를 사용할 수 있습니다.\n",
        "\n",
        "아래 예제에서, 주어진(original) 데이터 배치의 텍스트 항목들은 리스트(list)에 담긴(pack) 뒤 ``nn.EmbeddingBag`` 의 입력을 위한 하나의 tensor로 합쳐(concatenate)집니다.\n",
        "오프셋(offset)은 텍스트 tensor에서 개별 시퀀스 시작 인덱스를 표현하기 위한 구분자(delimiter) tensor입니다.\n",
        "레이블(label)은 개별 텍스트 항목의 레이블을 저장하는 tensor입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "M1kQHARe5diI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "2bg-Pc-95aNJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "offsets는 뭔가요?\n",
        "\n",
        "offsets는 미니배치 내 각 데이터 샘플의 시작 위치(index)를 기록하는 리스트입니다. 여기서는 초기에 [0]으로 시작하며, 각 데이터 샘플의 길이를 누적하여 저장합니다.\n",
        "cumsum은 뭔가요?\n",
        "\n",
        "cumsum은 누적 합(cumulative sum)을 계산하는 함수입니다. 여기서는 torch.tensor(offsets[:-1]).cumsum(dim=0)로 사용되어, offsets 리스트의 누적 합을 계산합니다.\n",
        "dim은 뭔가요?\n",
        "\n",
        "dim은 차원(dimension)을 지정하는 매개변수입니다. dim=0은 첫 번째 차원(행 방향)을 따라 누적 합을 계산하라는 의미입니다.\n",
        "collate_fn=collate_batch는 뭔가요?\n",
        "\n",
        "PyTorch의 DataLoader 클래스에서 collate_fn 매개변수는 데이터 배치를 어떻게 정렬할지를 지정하는 함수입니다. 주어진 미니배치의 요소를 받아 사용자 지정 함수 collate_batch를 사용하여 정리하고 반환합니다. 이렇게 함으로써, 각 미니배치가 모델에 입력될 수 있도록 데이터를 적절하게 처리하고 변환하는 역할을 합니다.\n",
        "이 코드는 특히 텍스트 데이터를 처리하는 데 유용한 기능을 제공하며, 레이블과 텍스트 데이터를 텐서로 변환하고, 누적 길이(offsets)를 기록하여 미니배치를 처리하는 방법을 정의합니다."
      ],
      "metadata": {
        "id": "6YgWUINP840B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의하기\n",
        "\n",
        "모델은\n",
        "[nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag)_\n",
        "레이어와 분류(classification) 목적을 위한 선형 레이어로 구성됩니다.\n",
        "기본 모드가 \"평균(mean)\"인 ``nn.EmbeddingBag`` 은 임베딩들의 \"가방(bag)\"의 평균 값을 계산합니다.\n",
        "이때 텍스트(text) 항목들은 각기 그 길이가 다를 수 있지만, ``nn.EmbeddingBag`` 모듈은 텍스트의 길이를\n",
        "오프셋(offset)으로 저장하고 있으므로 패딩(padding)이 필요하지는 않습니다.\n",
        "\n",
        "덧붙여서, ``nn.EmbeddingBag`` 은 임베딩의 평균을 즉시 계산하기 때문에,\n",
        "tensor들의 시퀀스를 처리할 때 성능 및 메모리 효율성 측면에서의 장점도\n",
        "갖고 있습니다.\n",
        "\n",
        "<img src=\"file://../_static/img/text_sentiment_ngrams_model.png\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w70SeFFw6-kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "aibWEqRb6kfm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "init_weights 함수:\n",
        "\n",
        "이 함수는 모델의 가중치를 초기화합니다.\n",
        "1. initrange는 초기화할 가중치의 범위를 나타냅니다.\n",
        "- self.embedding.weight.data.uniform_(-initrange, initrange): 임베딩 가중치를 초기화하며, 해당 가중치는 균등 분포에서 무작위로 선택됩니다.\n",
        "- self.fc.weight.data.uniform_(-initrange, initrange): 선형 레이어 가중치를 초기화합니다.\n",
        "- self.fc.bias.data.zero_(): 선형 레이어의 편향(bias)을 0으로 초기화합니다.\n",
        "\n",
        "2. uniform_:\n",
        "- uniform_은 PyTorch 텐서에 사용되는 함수로, 텐서의 값을 주어진 범위에서 무작위로 초기화합니다. 여기서는 균등 분포에서 초기화됩니다.\n",
        "\n",
        "3. -initrange, initrange:\n",
        "- 이 부분은 균등 분포의 범위를 지정합니다. 가중치가 -initrange에서 initrange 사이의 값으로 초기화됩니다.\n",
        "\n",
        "4. fc.bias.data.zero_():\n",
        "- fc는 fully connected(선형) 레이어를 나타냅니다.\n",
        "- bias는 레이어의 편향(bias)을 나타냅니다.\n",
        "- data.zero_()는 편향을 0으로 초기화합니다."
      ],
      "metadata": {
        "id": "gKaKpFnG9TbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인스턴스 생성하기\n",
        "\n",
        "``AG_NEWS`` 데이터셋에는 4종류의 레이블이 존재하므로 클래스의 개수도 4개입니다.\n",
        "\n",
        "::\n",
        "\n",
        "   1 : World (세계)\n",
        "   2 : Sports (스포츠)\n",
        "   3 : Business (경제)\n",
        "   4 : Sci/Tec (과학/기술)\n",
        "\n",
        "임베딩 차원이 64인 모델을 만듭니다.\n",
        "어휘집의 크기(Vocab size)는 어휘집(vocab)의 길이와 같습니다.\n",
        "클래스의 개수는 레이블의 개수와 같습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r61wrfWY9j84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "metadata": {
        "id": "VxfKzDGQ7oqk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델을 학습하고 결과를 평가하는 함수 정의하기\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UfhhaLz94HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "UaeBT2EQ9zPf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋을 분할하고 모델 수행하기\n",
        "\n",
        "원본 ``AG_NEWS`` 에는 검증용 데이터가 포함되어 있지 않기 때문에, 우리는 학습\n",
        "데이터를 학습 및 검증 데이터로 분할하려 합니다. 이때 데이터를 분할하는\n",
        "비율은 0.95(학습)와 0.05(검증) 입니다. 우리는 여기서 PyTorch의\n",
        "핵심 라이브러리 중 하나인\n",
        "[torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)_\n",
        "함수를 사용합니다.\n",
        "\n",
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)_\n",
        "기준(criterion)은 각 클래스에 대해 ``nn.LogSoftmax()`` 와 ``nn.NLLLoss()`` 를\n",
        "합쳐놓은 방식입니다.\n",
        "[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)_\n",
        "optimizer는 확률적 경사 하강법을 구현해놓은 것입니다. 처음의 학습률은\n",
        "5.0으로 두었습니다. 매 에폭을 진행하면서 학습률을 조절할 때는\n",
        "[StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)_\n",
        "을 사용합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2xO6_8bP_RX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K1Nq7er_FW8",
        "outputId": "fb41c907-ee82-4045-9a54-e70768e878c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.688\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.855\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.876\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 13.45s | valid accuracy    0.879 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.899\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.898\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 10.24s | valid accuracy    0.889 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.915\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  9.39s | valid accuracy    0.897 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.925\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.922\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 10.37s | valid accuracy    0.905 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.932\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.927\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 10.82s | valid accuracy    0.902 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.943\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.942\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.943\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  9.94s | valid accuracy    0.908 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.946\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.943\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.942\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  8.92s | valid accuracy    0.908 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.946\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.946\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.946\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 10.21s | valid accuracy    0.909 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.947\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.945\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.947\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  9.91s | valid accuracy    0.909 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.944\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.948\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.946\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  9.55s | valid accuracy    0.909 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가 데이터로 모델 평가하기\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0vYKdTYAwag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가 데이터셋을 통한 결과를 확인합니다...\n",
        "\n"
      ],
      "metadata": {
        "id": "yTyKEc_bAyA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBQKtJKeAu9K",
        "outputId": "6ca13fba-fc35-47d8-d6cc-5998b88d1556"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 임의의 뉴스로 평가하기\n",
        "\n",
        "현재까지 최고의 모델로 골프 뉴스를 테스트해보겠습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3YJJTPnA00p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSDAhfOxAzQ5",
        "outputId": "adf5410d-cba3-4468-de59-ca43c78343f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ]
    }
  ]
}