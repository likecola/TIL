{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXbkTfsjWgmC"
      },
      "source": [
        "\n",
        "# ``nn.Transformer`` 와 torchtext로 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델링하기\n",
        "\n",
        "이 튜토리얼에서는\n",
        "[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ 모듈을\n",
        "이용하는 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델을 학습하는 방법을 배워보겠습니다.\n",
        "\n",
        "PyTorch 1.2 버젼에는 [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_ 논문에\n",
        "기반한 표준 트랜스포머(transformer) 모듈을 포함하고 있습니다.\n",
        "트랜스포머 모델은 다양한 시퀀스-투-시퀀스 문제들에서 더 병렬화(parallelizable)가 가능하면서도\n",
        "순환 신경망(RNN; Recurrent Neural Network)과 비교하여 더 나은 성능을 보임이 입증되었습니다.\n",
        "``nn.Transformer`` 모듈은 입력(input) 과 출력(output) 사이의 전역적인 의존성(global dependencies)\n",
        "을 나타내기 위하여 ([nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)_ 으로\n",
        "구현된) 어텐션(attention) 메커니즘에 전적으로 의존합니다.\n",
        "현재 ``nn.Transformer`` 모듈은 모듈화가 잘 되어 있어\n",
        "단일 컴포넌트 (예. [nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)_ )\n",
        "로 쉽게 적용 및 구성할 수 있습니다.\n",
        "\n",
        "<img src=\"file://../_static/img/transformer_architecture.jpg\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS8oDqZyXLzi"
      },
      "source": [
        "## 모델 정의하기\n",
        "\n",
        "\n",
        "이 튜토리얼에서, 우리는 ``nn.TransformerEncoder`` 모델을 언어 모델링(language modeling) 과제에 대해서 학습시킬 것입니다.\n",
        "언어 모델링 과제는 주어진 단어 (또는 단어의 시퀀스) 가 다음에 이어지는 단어 시퀀스를 따를 가능성(likelihood)에 대한 확률을 할당하는 것입니다.\n",
        "먼저, 토큰(token) 들의 시퀀스가 임베딩(embedding) 레이어로 전달되며, 이어서 포지셔널 인코딩(positional encoding) 레이어가 각 단어의 순서를 설명합니다.\n",
        "(더 자세한 설명은 다음 단락을 참고해주세요.)\n",
        "``nn.TransformerEncoder`` 는 여러 개의\n",
        "[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)_\n",
        "레이어로 구성되어 있습니다.\n",
        "``nn.TransformerEncoder`` 내부의 셀프-어텐션(self-attention) 레이어들은 시퀀스 안에서의 이전 포지션에만 집중하도록 허용되기 때문에,\n",
        "입력(input) 순서와 함께, 정사각 형태의 어텐션 마스크(attention mask) 가 필요합니다.\n",
        "언어 모델링 과제를 위해서, 미래의 포지션에 있는 모든 토큰들은 마스킹 되어야(가려져야) 합니다.\n",
        "실제 단어를 얻기 위해서, ``nn.TransformerEncoder`` 의 출력은 로그-소프트맥스(log-Softmax) 로 이어지는 최종 선형(Linear) 레이어로 전달 됩니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CdPdxZuJWdtN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPoXAqjccpqz"
      },
      "source": [
        "``PositionalEncoding`` 모듈은 시퀀스 안에서 토큰의 상대적인 또는 절대적인 포지션에 대한 어떤 정보를 주입합니다.\n",
        "포지셔널 인코딩은 임베딩과 합칠 수 있도록 똑같은 차원을 가집니다.\n",
        "여기에서, 우리는 다른 주파수(frequency) 의 ``sine`` 과 ``cosine`` 함수를 사용합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QGoXBbuVcoKD"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-anaxhNdAVv"
      },
      "source": [
        "## 데이터 로드하고 배치 만들기\n",
        "\n",
        "\n",
        "이 튜토리얼에서는 ``torchtext`` 를 사용하여 Wikitext-2 데이터셋을 생성합니다.\n",
        "torchtext 데이터셋에 접근하기 전에, https://github.com/pytorch/data 을 참고하여 torchdata를 설치하시기 바랍니다.\n",
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zKDNRXFc-5D",
        "outputId": "d2d297b2-6b57-4aae-da29-d7b894b09d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install torchdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXofpwfldFia"
      },
      "source": [
        "어휘(vocab) 객체는 훈련 데이터셋(train dataset) 에 의하여 만들어지고, 토큰(token)을 텐서(tensor)로 수치화하는데 사용됩니다.\n",
        "Wikitext-2에서 보기 드문 토큰(rare token)은 `<unk>` 로 표현됩니다.\n",
        "\n",
        "주어진 1D 벡터의 시퀀스 데이터에서, ``batchify()`` 함수는 데이터를 ``batch_size`` 컬럼들로 정렬합니다.\n",
        "만약 데이터가 ``batch_size`` 컬럼으로 나누어 떨어지지 않으면, 데이터를 잘라내서 맞춥니다.\n",
        "예를 들어 (총 길이 26의) 알파벳을 데이터로 보고 ``batch_size=4`` 일 때, 알파벳은 길이가 6인 4개의 시퀀스로 나눠집니다:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "배치 작업(batching)은 더 많은 병렬 처리를 가능하게 하지만, 모델이 독립적으로 각 컬럼들을 취급해야 함을 뜻합니다;\n",
        "예를 들어, 위 예제에서 ``G`` 와 ``F`` 의 의존성(dependance)은 학습되지 않습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P3BCNrGdSka",
        "outputId": "2f6c935b-e861-42f7-b0fa-15372f866602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ValeouIEdDjS"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Arguments:\n",
        "        data: Tensor, shape ``[N]``\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape ``[N // bsz, bsz]``\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XceS6SeKdxXS"
      },
      "source": [
        "### 입력(input) 과 타겟(target) 시퀀스를 생성하기 위한 함수들\n",
        "\n",
        "\n",
        "``get_batch()`` 함수는 트랜스포머 모델을 위한 입력-타겟 시퀀스 쌍(pair)을 생성합니다.\n",
        "이 함수는 소스 데이터를 ``bptt`` 길이를 가진 덩어리로 세분화 합니다.\n",
        "언어 모델링 과제를 위해서, 모델은 다음 단어인 ``Target`` 이 필요 합니다.\n",
        "예를 들어, ``bptt`` 의 값이 2 라면, 우리는 ``i`` = 0 일 때 다음의 2 개의 변수(Variable) 를 얻을 수 있습니다:\n",
        "\n",
        "<img src=\"file://../_static/img/transformer_input_target.png\">\n",
        "\n",
        "변수 덩어리는 트랜스포머 모델의 ``S`` 차원과 일치하는 0 차원에 해당합니다.\n",
        "배치 차원 ``N`` 은 1 차원에 해당합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KlJQhc8qdzHi"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple ``(data, target)``, where data has shape ``[seq_len, batch_size]`` and\n",
        "        target has shape ``[seq_len * batch_size]``\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w96eNVC9d3ci"
      },
      "source": [
        "## 인스턴스(instance) 초기화하기\n",
        "\n",
        "\n",
        "모델의 하이퍼파라미터(hyperparameter)는 아래와 같이 정의됩니다.\n",
        "어휘집( ``vocab`` )의 크기는 단어 오브젝트의 길이와 일치 합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwAvyQJwd1Vy",
        "outputId": "8c76d2bf-8f3d-45d5-c866-293ff9638d7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "ntokens = len(vocab) # 단어 사전(어휘집)의 크기\n",
        "emsize = 200 # 임베딩 차원\n",
        "d_hid = 200 # ``nn.TransformerEncoder`` 에서 피드포워드 네트워크(feedforward network) 모델의 차원\n",
        "nlayers = 2 # ``nn.TransformerEncoder`` 내부의 nn.TransformerEncoderLayer 개수\n",
        "nhead = 2 # ``nn.MultiheadAttention`` 의 헤드 개수\n",
        "dropout = 0.2 # 드랍아웃(dropout) 확률\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIr21y9gd7UL"
      },
      "source": [
        "## 모델 실행하기\n",
        "\n",
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)_ 를\n",
        "[SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)_ (확률적 경사 하강법) 옵티마이저(optimizer)와\n",
        "함께 사용하였습니다. 학습률(learning rate)는 5.0으로 초기화하였으며 [StepLR](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR)_\n",
        "스케쥴을 따릅니다. 학습하는 동안, [nn.utils.clip_grad_norm\\_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)_\n",
        "을 사용하여 기울기(gradient)가 폭발(exploding)하지 않도록 합니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZLIWNleyd56i"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # 학습률(learning rate)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # 학습 모드 시작\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # 마지막 배치에만 적용\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # 평가 모드 시작\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapQAcIQeIbq"
      },
      "source": [
        "에포크 내에서 반복됩니다. 만약 검증 오차(validation loss) 가 우리가 지금까지 관찰한 것 중 최적이라면 모델을 저장합니다.\n",
        "매 에포크 이후에 학습률을 조절합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m2yvDfeeHMD",
        "outputId": "83f406ca-3f19-47bb-f041-e6a2ebda686d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 35.97 | loss  8.13 | ppl  3409.89\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 13.59 | loss  6.86 | ppl   950.85\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 13.74 | loss  6.44 | ppl   627.39\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 14.31 | loss  6.31 | ppl   547.81\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 13.91 | loss  6.19 | ppl   486.45\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 13.95 | loss  6.16 | ppl   474.73\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 13.90 | loss  6.12 | ppl   454.31\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 14.12 | loss  6.11 | ppl   451.67\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 14.46 | loss  6.02 | ppl   412.48\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 14.09 | loss  6.02 | ppl   411.47\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 14.13 | loss  5.90 | ppl   366.07\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 14.25 | loss  5.97 | ppl   391.68\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 14.56 | loss  5.95 | ppl   384.13\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 14.71 | loss  5.89 | ppl   360.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 47.66s | valid loss  5.78 | valid ppl   324.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 14.59 | loss  5.86 | ppl   351.61\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 15.28 | loss  5.85 | ppl   348.17\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 17.03 | loss  5.67 | ppl   290.80\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 14.45 | loss  5.71 | ppl   301.85\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 14.43 | loss  5.66 | ppl   285.74\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 14.36 | loss  5.69 | ppl   294.81\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 14.61 | loss  5.69 | ppl   296.20\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.51 | loss  5.71 | ppl   302.85\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.21 | loss  5.65 | ppl   285.58\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.23 | loss  5.67 | ppl   290.46\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 14.14 | loss  5.55 | ppl   257.54\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.52 | loss  5.65 | ppl   283.05\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 14.26 | loss  5.64 | ppl   282.70\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.09 | loss  5.57 | ppl   263.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 44.58s | valid loss  5.63 | valid ppl   277.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 14.40 | loss  5.60 | ppl   270.42\n",
            "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.44 | loss  5.62 | ppl   274.87\n",
            "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.09 | loss  5.42 | ppl   226.94\n",
            "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.07 | loss  5.48 | ppl   240.56\n",
            "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.10 | loss  5.43 | ppl   228.89\n",
            "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 14.45 | loss  5.48 | ppl   240.29\n",
            "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 14.38 | loss  5.49 | ppl   242.93\n",
            "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.13 | loss  5.53 | ppl   251.20\n",
            "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 14.16 | loss  5.47 | ppl   236.52\n",
            "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.14 | loss  5.49 | ppl   241.64\n",
            "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 14.63 | loss  5.36 | ppl   212.99\n",
            "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.39 | loss  5.46 | ppl   235.95\n",
            "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.22 | loss  5.47 | ppl   236.70\n",
            "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.24 | loss  5.41 | ppl   223.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 43.94s | valid loss  5.59 | valid ppl   267.49\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "\n",
        "with TemporaryDirectory() as tempdir:\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model)\n",
        "        val_loss = evaluate(model, val_data)\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFlhSQlReKpz"
      },
      "source": [
        "## 평가 데이터셋(test dataset)으로 모델을 평가하기\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOJ8n0Q4eK_i",
        "outputId": "35d004c3-8973-4ed8-88ff-fa711be18bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.50 | test ppl   243.88\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
