{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [자연어 용어정리] 언어 모델, 셀프 어텐션, BERT와 GPT, 벡터\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## 언어 모델(language model)\n",
    "\n",
    "단어 시퀀스에 확률을 부여하는 모델입니다.\n",
    "\n",
    "다시 말해 시퀀스를 입력 받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 모델입니다.\n",
    "\n",
    "\n",
    "\n",
    "이전 단어를이 주어졌을 때 다음 단어가 나타날 확률을 부여하는 모델\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## 순방향 언어 모델\n",
    "\n",
    "\n",
    "문장 앞부터 뒤로, 사람이 이해하는 순서대로 계산하는 모델을\n",
    "\n",
    "순방향 언어 모델이라고 합니다.\n",
    "\n",
    "GPT, ELMo 모델이 이런 방식으로 프리트레인을 수행합니다.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## 역방향 언어 모델\n",
    "\n",
    "\n",
    "문장 뒤부터 앞으로 계산하는 모델입니다.\n",
    "\n",
    "ELMo 같은 모델이 이런 방식으로 프리트레인을 수행합니다. ( ELMo 모델은 순방향, 역방향을 모두 활용합니다)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 마스크 언어 모델\n",
    "\n",
    "학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정으로 학습합니다.\n",
    "\n",
    "BERT 가 대표적인 마스크 언어 모델입니다.\n",
    "\n",
    "맟힐 단어를 계산할 때 문장 전체의 맥락을 참고할 수 있다는 장점이 있습니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 스킵-그램 모델\n",
    "\n",
    "어떤 단어 앞뒤에 특정 범위를 정해 두고 이 범위 내에 어떤 단어들이 올지 분류하는 과정으로 학습합니다.\n",
    "\n",
    "컨텍스트로 설정한 단어 주변에 어떤 단어들이 분포해 있는지 학습합니다.\n",
    "\n",
    "\n",
    "\n",
    "**언어 모델이 주목받는 이유 가운데 하나는 데이터 제작 비용 때문입니다.**\n",
    "\n",
    "'다음 단어 맞히기'나 '빈칸 맞히기' 등으로 학습 태스크를 구성하면 사람이 일일이 수작업해야 하는 \n",
    "\n",
    "레이블 없이도 많은 학습 데이터를 싼값에 만들어 낼 수 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 시퀀스-투-시퀀스\n",
    "\n",
    "시퀀스란 단어 같은 무언가의 나열을 의미합니다.\n",
    "\n",
    "특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업을 가리킵니다.\n",
    "\n",
    "소스와 타깃의 길이가 달라도 해당과제를 수행하는 데 문제가 없어야 합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 인코더와 디코더\n",
    "\n",
    "인코더는 소스 시퀀스의 정보를 압축해 디코더로 보내는 역할을 담당합니다.\n",
    "\n",
    "인코더가 소스 시퀀스 정보를 압축하는 과정을 인코딩이라고 합니다.\n",
    "\n",
    "\n",
    "\n",
    "디코더는 인코더가 보내준 소스 시퀀스 정보를 받아서 타깃 시퀀스를 생성합니다.\n",
    "\n",
    "디코더가 타깃 시퀀스를 생성하는 과정을 디코딩이라고 합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 트랜스포머\n",
    "\n",
    "디코더 출력은 타깃 언어의 어휘 수 만큼의 차원으로 구성된 벡터입니다.\n",
    "\n",
    "이 벡터의 특징은 요소(element)값이 모두 확률이라는 점입니다.\n",
    "\n",
    "타깃 언어의 어휘가 총 3만 개라고 가정하면 디코더 출력은 3만 차원의 벡터입니다.\n",
    "\n",
    "\n",
    "\n",
    "트랜스포머는 인코더와 디코더 입력이 주어졌을 때\n",
    "\n",
    "정답에 해당하는 단어의 확률값을 높이는 방식으로 학습합니다.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## 어텐션(Attention)\n",
    "\n",
    "시퀀스 요소 가운데 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 \n",
    "\n",
    "태스크 수행 성능을 끌어 올리는 방식입니다.\n",
    "\n",
    "\n",
    "\n",
    "셀프어텐션이란, 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출합니다.\n",
    "\n",
    "말 그대로 자신에게 수행하는 어텐션 기법입니다.\n",
    "\n",
    "셀프 어텐션 수행 대상은 입력 시퀀스 전체입니다.\n",
    "\n",
    "\n",
    "\n",
    "어텐션은 소스 시퀀스 전체 단어들 사이를 연결하는 데 쓰입니다.\n",
    "\n",
    "반면 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결합니다.\n",
    "\n",
    "\n",
    "\n",
    "셀프 어텐션은 RNN 없이 동작합니다.\n",
    "\n",
    "타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 \n",
    "\n",
    "셀프 어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행합니다.\n",
    "\n",
    "\n",
    "\n",
    "셀프 어텐션은 쿼리, 키, 밸류 3가지 요소가 서로 영향을 주고받는 구조입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "쿼리와 키를 행렬곱한 뒤 해당 행렬의 모든 요솟값을 키 차원 수의 제곱근으로 나누고,\n",
    "\n",
    "이 행렬을 행 단위로 소프트맥스를  취해 스코어 행렬로 만들어 줍니다.\n",
    "\n",
    "이 스코어 행렬에 밸류를 행렬곱하면 셀프 어텐션 계산을 마칩니다.\n",
    "\n",
    "\n",
    "\n",
    "소프트맥스란 입력 벡터의 모든 요솟값 범위를 0~1로 하고 총합이 1이 되게끔 하는 함수입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**수정예정**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
