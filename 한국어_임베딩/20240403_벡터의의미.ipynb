{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 계산과 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어의 통계적 패턴(statisical pattern)정보를 통쨰로 임베딩에 넣는 것이 본질입니다.\n",
    "\n",
    "**임베딩 만들 때 쓰는 통계 정보**\n",
    "- 문장에 어떤 안어가 많이 쓰였는가\n",
    "- 단어가 어떤 순서로 등장하는지\n",
    "- 문장에 어떤 단어가 나왔는지\n",
    "\n",
    "**백오브워즈(bag of words)**\n",
    "- 어떤 단어가 많이 쓰였는지 정보를 중시한다.\n",
    "저자의 의도는 단어 사용 여부나 그 빈도에서 드러난다고 보기 때문이다.   \n",
    "단어의 순서정보는 무시한다. 백오브워즈 자겅에서 가장 많이 쓰이는 통계량은 TF-IDF이다.\n",
    "\n",
    "\n",
    "단어의 등장 순서를 무시하는 백오브워즈의 대척점에는 언어모델이 있다.   \n",
    "언어 모델은 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러울지 확률을 부여한다.\n",
    "\n",
    "\n",
    "**분포 가정(distributional hyothesis)**\n",
    "문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다.  \n",
    "단어의 의미는 그 주변 문맥(context)을 통해 유추해볼 수 있다고 보는 것이다.  \n",
    "분포 가정의 대표 통계량은 PMI(Pointwise Mutual Information)이며 대표 모델은 Word2Vec이다.\n",
    "\n",
    "\n",
    "언어 모델에서는 단어의 등장 순서를, 분포 가정에서는 이웃 단어(문맥)을 우선시한다.   \n",
    "어떤 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 떼려야 뗄 수 없는 관계를 가진다.\n",
    "\n",
    "\n",
    "한편 분포 가정에서는 어떤 단어 쌍이 얼마나 같이 자주 나타나는지와 관련한 정보를 수치화 하기 위해서   \n",
    "개별 단어 그리고 단어 쌍의 빈도 정보를 적극 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떤 단어가 많이 쓰였는가\n",
    "\n",
    "### 백오브워즈 가정\n",
    "- 백이란 중복 원소를 허용한 집합을 뜻한다.  \n",
    "원소의 순서는 고려하지 않는다.\n",
    "\n",
    "자연어 처리 분야에서 백오브워즈란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 말한다.  \n",
    "문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것이라고 보면 된다.\n",
    "경우에 따라서 빈도를 단순화해서 등장여부만 쓰기도 한다.\n",
    "\n",
    "백오브워즈 임베딩에서는 '저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다'라는 가정이 깔려있다.  \n",
    "주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷할 것이고,   \n",
    "백오브워즈 임베딩 역시 유사할 것이라고 보는 것이다.\n",
    "\n",
    "정보검색(Information retrieval)분야에서 여전히 많이 사용된다.\n",
    "사용자의 query에 가장 적절한 문서를 보여줄 때 query를 백오브워즈 임베딩으로 변환하고   \n",
    "질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 유사도가 가장 높은 문서를 사용자에게 노출한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### TF-IDF\n",
    "을/를 등의 조사가 많이 등장해서 단어 빈도로만 임베딩으로 하기에는 어려움이 있다.      \n",
    "그래서 TF-IDF 가 등장했는데 가중치를 계산해 행렬 원소를 바꾼다.  \n",
    "TF는 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지 빈도를 나타낸다.  \n",
    "DF는 특정 단어가 나타난 문서의 수를 뜻한다.  \n",
    "IDF는 전체 문서 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값이다. (그 값이 클수록 특이한 단어라는 뜻이다)     \n",
    "이는 단어의 주제 예측 능력과 직결된다.   \n",
    "\n",
    "어떤 단어의 주제 예측 능력이 강할 수록 가중치가 커지고 그 반대의 경우 작아진다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Deep Averaging Network\n",
    "Deep Averaging Network는 백오브워즈 가정의 뉴럴 네트워크 버전이다.   \n",
    "문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지 그 빈도만을 따진다.   \n",
    "문장 임베딩을 입력 받아 해당 문서가 어떤 범주인지 분류한다.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어가 어떤 순서로 쓰였는가\n",
    "<br>\n",
    "\n",
    "### 통계 기반 모델\n",
    "\n",
    "언어 모델이란 단어 시퀀스에 확률을 부여하는 모델이다.   \n",
    "단어의 등장 순서를 무시하는 백오브워즈와 달리 **언어 모델은 시퀀스 정보를 명시적으로 학습한다.**   \n",
    "따라서 백오브워즈와 달리 언어 모델은 시퀀스 정보를 명시적으로 학습한다.\n",
    "\n",
    "통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습니다.   \n",
    "\n",
    "**n-gram**이란 n개의 단어를 뜻하는 용어다.  \n",
    "2-gram 또는 바이그램(bigram)이다.  \n",
    "경우에 따라서 n-gram은 n-gram에 기반한 언어 모델을 의미하기도 한다.   \n",
    "말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 뜻이다.\n",
    "\n",
    "\n",
    "- 마코프 가정(Markov assumption) : 한 상태(state)의 확률은 그 직전 상태에만 의존한다.\n",
    "- 백오프(back-off) : n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식이다.\n",
    "- 스무딩(smoothing) : 스무딩을 시행하면 높은 빈도를 가진 문자열의 등장 확률을 일부 깎고 학습 데이터에 전혀 등장하지 않는 케이스들에는 작으나마 일부 확률을 부여하게된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴럴 네트워크 기반 언어모델\n",
    "- 뉴럴 네트워크는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로 가능할 수 있기 때문이다.   \n",
    "주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 (prediction)과정에서 학습된다.   \n",
    "학습이 완료되면 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용한다. (ELMo, GPT)  \n",
    "\n",
    "**마스크 언어 모델**\n",
    "- 모델 기반 기법과 큰 틀에서 유사하지만 디테일에서 차이를 보인다.   \n",
    "문장 중간에 마스크를 씌워 놓고, 해당 마스크 위치에 어떤 단어가 올지 예측하는 과정에서 학습한다.   \n",
    "\n",
    "언어 모델 기반 기법은 단어를 순차적으로 입력받아 다음 단어를 맞춰야 하기 때문에 태생적으로 일방향이다.   \n",
    "하지만 마스크 언어 모델 기반 기법은 문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 양방향 학습이 가능하다. (BERT 모델)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어떤 단어가 같이 쓰였는가\n",
    "\n",
    "### 분포 가정\n",
    "- 자연어 처리에서 분포(distribution)란 특정 범위, 즉 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 가리킨다.   \n",
    "개별 단어의 분포는 그 단어가 문장 내에서 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라진다.   \n",
    "어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장했다면 그 의미 또한 유사할 것이라는 게 분포가정의 전제다.\n",
    "\n",
    "\n",
    "\"단어의 의미는 곧 그 언어에서의 활용이다\" 라는 철학에 기반해있다.   \n",
    "\n",
    "\n",
    "### 분포와 의미(1) : 형태소\n",
    "- 언어학에서 형태소란 \"의미를 가지는 최소 단위\"를 말한다.   \n",
    "이때 의미는 어휘적인 것뿐만 아니라 문법적인 것도 포함된다.   \n",
    "<br>\n",
    "\n",
    "**계열관계(paradigmatic relation)**는 해당 형태소 자리에 다른 형태소가 '대치'돼 쓰일 수 있는가를 따지는 것이다.\n",
    "\n",
    "\n",
    "### 분포와 의미(2) : 품사\n",
    "- 품사란 단어를 문법적 성질의 공통성에 따라 언어학자들이 몇 갈래로 묶어 놓은 것이다.   \n",
    "- 품사 분류의 기준은 기능, 의미, 형식 등 세 가지다.   \n",
    "\n",
    "<br>\n",
    "\n",
    "### 점별 상호 정보량(PMI, Pointwise Mutual Information)\n",
    "- 두 확률변수 사이의 상관성을 계량화하는 단위다.   \n",
    "- 두 확률변수가 완전히 독립인 경우 그 값이 0이 된다.   \n",
    "- PMI는 분포 가정에 따른 단어 가중치 할당 기법이다.   \n",
    "두 단어가 얼마나 자주 같이 등장하는지에 관한 정보를 수치화한 것이기 때문이다.   \n",
    "- 이렇게 구축한 PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수도 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Word2Vec\n",
    "- 분포 가정의 대표적인 모델이다.   \n",
    "- COBW 모델은 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습된다.   \n",
    "- Skip-gram 모델은 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다.   \n",
    "- 둘 모두 특정 타깃 단어 주변의 문맥, 즉 분포 정보를 임베딩에 함축한다.\n",
    "\n",
    "\n",
    "#### 요약\n",
    "임베딩에 자연어의 통계적 패턴정보를 주면 자연어의 의미를 함축할 수 있다.  \n",
    "백오브워즈 가정에서는 어떤 단어의 등장여부 혹은 그 빈도 정보를 중시한다 (단 순서정보는 무시한다)   \n",
    "백오브워즈 가정의 대척점에는 언어 모델이 있다. 언어 모델은 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다.   \n",
    "분포 가정에서는 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다.   \n",
    "백오브워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
