{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [딥러닝 용어정리] 자연어 처리\n",
    "\n",
    "\n",
    "### 트랜스퍼 러닝(Transfer learning)\n",
    "\n",
    "특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용 하는 기법\n",
    "\n",
    "모델이 태스크를 수행해봤던 경험을 재활용한다.\n",
    "\n",
    "기존보다 모델의 학습속도가 빨라지고 더 잘 수행하는 경향이 있다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 업스트림 태스크\n",
    "\n",
    "자연어의 풍부한 문맥을 모델에 내재화하고 모델을 다양한 다운스트림 태스크에 적용해 성능을 대폭 끌어올림\n",
    "\n",
    "대표적인 업스트림 태스크 중 하나는 \"다음 단어 맞히기\"이다.\n",
    "\n",
    "\n",
    "\n",
    "모델이 대규모 말뭉치를 가지고 과정을 수행하면 \n",
    "\n",
    "이전 문맥을 고려했을 때 어떤 단어가 그 다음에 오는 것이 자연스러운지 알 수 있게 됩니다.\n",
    "\n",
    "\n",
    "\n",
    "언어 모델에서는 학습 대상 언어의 어휘 수만큼 분류할 범주가 늘어납니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "또 다른 업스트림 태스크로는 \"빈칸 채우기\"가 있습니다.\n",
    "\n",
    "BERT 계열 모델이 이 태스크로 프리트레인을 수행합니다.\n",
    "\n",
    "\n",
    "\n",
    "모델이 많은 양의 데이터를 가지고 빈칸 채우기를 반복하면\n",
    "\n",
    "앞 뒤 문맥을 보고 빈칸에 적합한 단어를 알 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "이처럼 빈칸 채우기로 업스트림 태스크를 수행한 모델을 마스크 언어 모델이라고 합니다.\n",
    "\n",
    "마스크 언어 모델의 학습 역시 모델과 비슷합니다.\n",
    "\n",
    "업스트림 태스크를 수행한 모델은 성능이 기존보다 월등히 좋아졌습니다.\n",
    "\n",
    "\n",
    "\n",
    "데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기지도 학습이라고 합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 다운스트림 태스크\n",
    "\n",
    "다운스트림 태스크는 우리가 풀어야 할 자연어 처리의 구체적인 과제 입니다.\n",
    "\n",
    "파인튜닝은 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트 하는 기법입니다.\n",
    "\n",
    "\n",
    "\n",
    "**문서분류** \n",
    "\n",
    "- 문서 분류 모델은 자연어를 입력받아 해당 입력이 어떤 범주에 속하는지 그 확률값을 반환합니다.\n",
    "\n",
    "프리트레인을 마친 마스크 언어 모델 위에 작은 모듈을 하나 더 쌓아 문서 전체의 범주를 분류합니다.\n",
    "\n",
    "\n",
    "\n",
    "**자연어 추론**\n",
    "\n",
    "- 자연어 추론 모델은 문장 2개를 입력받아 두 문장 사이의 관계가 참, 거짓, 중립 등 어떤 범주인지 그 확률 값을 반환합니다.\n",
    "\n",
    "\n",
    "\n",
    "**개체명 인식**\n",
    "\n",
    "- 개체명 인식 모델은 자연어를 입력받아 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 그 확률값을 반환합니다.\n",
    "\n",
    "\n",
    "\n",
    "**질의응답**\n",
    "\n",
    "- 질의응답 모델은 자연어(질문 + 질문)를 입력받아 각 단어가 정답을 시작일 확률값과 끝일 확률 값을 반환합니다.\n",
    "\n",
    "\n",
    "\n",
    "**문장 생성**\n",
    "\n",
    "- 문장 생성 모델은 GPT 계열 언어 모델이 널리 쓰입니다.\n",
    "\n",
    "자연어(문장)를 입력받아 어휘 전체에 대한 확률 값을 반환합니다.\n",
    "\n",
    "이 확률 값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 다운스트림 태스크 학습 방식\n",
    "\n",
    "\n",
    "\n",
    "**파인튜닝(fine-tuning)**\n",
    "\n",
    ": 다운스트림 태스크 데이터 전체를 사용합니다.\n",
    "\n",
    "다운스트림 데이터에 맞게 모델 전체를 업데이트 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**프롬프트 튜닝(prompt tuning)**\n",
    "\n",
    ": 다운스트림 태스크 데이터 전체를 사용합니다.\n",
    "\n",
    "다운스트림 데이터에 맞게 모델 일부만 업데이트 합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**인컨텍스트 러닝(in-context learning)**\n",
    "\n",
    ": 다운스트림 태스크 데이터의 일부만 사용합니다.\n",
    "\n",
    "모델을 업데이트 하지 않습니다.\n",
    "\n",
    "\n",
    "\n",
    "* 파인 튜닝 이외의 방식이 주목받고 있는 이유는 비용과 성능 때문입니다.\n",
    "\n",
    "인컨텍스트 러닝에는 3가지 방식이 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "*제로샷 러닝(zero-shot learning)*\n",
    "\n",
    ": 다운스트림 태스크 데이터를 전혀 사용하지 않습니다.\n",
    "\n",
    "모델이 바로 다운스트림 태스크를 수행합니다.\n",
    "\n",
    "\n",
    "\n",
    "*원샷 러닝 (one-shot learning)*\n",
    "\n",
    ": 다운스트림 태스크 데이터를 1건만 사용합니다.\n",
    "\n",
    "모델은 1건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행합니다.\n",
    "\n",
    "\n",
    "\n",
    "*퓨샷 러닝(few-shot learning)*\n",
    "\n",
    ": 다운스트림 태스크 데이터를 몇 건만 사용합니다.\n",
    "\n",
    "모델은 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 하이퍼파라미터(hyperparameter)\n",
    "\n",
    ": 모델 구조와 학습 등에 직접 관계된 설정값을 가리킵니다.\n",
    "\n",
    "예를 들어 learning rate, batch size 등이 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "*Learning Rate (학습률):*\n",
    "- 정의: 학습률은 모델이 데이터에서 학습할 때 각 반복(에포크)에서 얼마나 많이 업데이트를 수행할지를 결정하는 파라미터입니다.\n",
    "- 간단한 설명: 학습률은 한 번의 학습 단계에서 모델이 얼마나 큰 \"걸음\"을 내딛을지를 결정합니다. 너무 작은 학습률은 학습을 느리게 만들 수 있고, 너무 큰 학습률은 모델이 수렴하지 못하고 발산할 수 있습니다.\n",
    "\n",
    "\n",
    "*Batch Size (배치 크기):*\n",
    "- 정의: 배치 크기는 한 번의 학습 단계에서 사용되는 데이터 샘플의 수를 나타냅니다.\n",
    "- 간단한 설명: 배치 크기는 모델이 한 번에 몇 개의 데이터를 처리할지를 결정합니다. 작은 배치 크기는 메모리 사용을 줄일 수 있고, 더 많은 업데이트를 허용합니다. 그러나 큰 배치 크기는 학습 속도를 높일 수 있지만 메모리 요구량이 많아집니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 토크나이저 준비\n",
    "\n",
    "토큰 : 자연어 처리 모델의 입력으로 문장보다 작은 단위\n",
    "문장을 띄어쓰기만으로 나눌 수 있고, 의미의 최소 단위인 형태소 단위로 나눌 수도 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "문장을 토큰 시퀀스로 분석하는 과정을 토큰화,\n",
    "\n",
    "토큰화를 수행하는 프로그램을 토크나이저라고 합니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 컬레이트(collate)\n",
    "\n",
    ": 배치의 모양 등을 정비해 모델의 최종 입력으로 만들어 주는 과정 \n",
    "\n",
    "컬레이트의 과정에는 파이썬 리스트에서 파이토치 텐서로의 변환 등 자로형 변환도 포함됩니다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 최적화 (optimization)\n",
    "\n",
    ": 특정 조건에서 어떤 값이 최대나 최소가 되도록 하는 과정을 가리킵니다.\n",
    "\n",
    "이를 위해 옵티마이저, 러닝 레이트 스케줄러 등을 정의해둡니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
